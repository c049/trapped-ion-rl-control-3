

===== PAGE 1 =====
Model-FreeQuantumControlwithReinforcementLearning
V.V.Sivak,
1,*A.Eickbusch,
1H.Liu,
1B.Royer,
2I.Tsioutsios,
1andM.H.Devoret
1,ƒ1DepartmentofAppliedPhysics,YaleUniversity,NewHaven,Connecticut06520,USA
2DepartmentofPhysics,YaleUniversity,NewHaven,Connecticut06520,USA
(Received3May2021;revised3December2021;accepted28January2022;published28March2022)
Modelbiasisaninherentlimitationofthecurrentdominantapproachtooptimalquantumcontrol,which
reliesonasystemsimulationforoptimizationofcontrolpolicies.Toovercomethislimitation,weproposea
circuit-basedapproachfortrainingareinforcementlearningagentonquantumcontroltasksinamodel-free
way.Givenacontinuouslyparametrizedcontrolcircuit,theagentlearnsitsparametersthroughtrial-and-

errorinteractionwiththequantumsystem,usingmeasurementoutcomesastheonlysourceofinformation
aboutthequantumstate.Focusingoncontrolofaharmonicoscillatorcoupledtoanancillaqubit,weshow
howtorewardthelearningagentwithmeasurementsofexperimentallyavailableobservables.Wetrainthe

agenttopreparevariousnonclassicalstatesviabothunitarycontrolandcontrolwithadaptive
measurement-basedquantumfeedback,andtoexecutelogicalgatesonencodedqubits.Theagentdoes
notrelyonaveragingforstatetomographyorfidelityestimation,andsignificantlyoutperformswidelyused

model-freemethodsintermsofsampleefficiency.Ournumericalworkisofimmediaterelevanceto
superconductingcircuitsandtrappedionsplatformswheresuchtrainingcanbeimplementedin
experiment,allowingcompleteeliminationofmodelbiasandtheadaptationofquantumcontrolpolicies

tothespecificsysteminwhichtheyaredeployed.
DOI:10.1103/PhysRevX.12.011059
SubjectAreas:QuantumInformation
I.INTRODUCTION
Quantumcontroltheoryaddressesaproblemofopti-
mallyimplementingadesiredquantumoperationusing
externalcontrols.Thedesignofexperimentalcontrol

policiesiscurrentlydominatedbysimulation-basedopti-
malcontroltheorymethods,withfavorableconvergence
propertiesthankstotheavailabilityofanalyticgradients

[1Œ3]orautomaticdifferentiation
[4,5]
.However,itis
importanttoacknowledgethatsimulation-basedmethods
canonlybeasgoodastheunderlyingmodelsusedinthe

simulation.Empirically,modelbiasleadstoasignificant
degradationofperformanceofthequantumcontrolpoli-
cies,whenoptimizedinsimulationandthentestedin

experiment
[6Œ9].Apracticalmodel-freealternativeto
simulation-basedmethodsinquantumcontrolisthus
desirable.Theideaofusingmodel-freeoptimizationinquantum
controlcanbetracedbacktothepioneeringproposalin
1992oflaserpulseshapingformolecularcontrolwitha
geneticalgorithm
[10].Onlyinrecentyearshasthe
controllabilityofquantumsystemsandthedutycycleof

optimizationfeedbackloopsreachedsufficientlevelsto

allowfortheexperimentalimplementationofsuchideas.
Thefewexistingdemonstrationsarebasedonmodel-free
optimizationalgorithmssuchastheNelder-Mead(NM)

simplexsearch
[6Œ8],evolutionarystrategies
[9],and
particleswarmoptimization
[11].Atthesametime,deepreinforcementlearning(RL)
[12,13]emergedasnotonlyapowerfuloptimization
techniquebutalsoatoolfordiscoveringadaptivedeci-

sion-makingpolicies.Inthisframework,learningproceeds

bytrialanderror,withoutaccesstothemodelgenerating

thedynamicsanditsgradients.Beingintrinsicallyfreeof

modelbias,itisanattractivealternativetotraditional
simulation-basedapproachesinquantumcontrol.Ina
varietyofdomains,deepreinforcementlearninghas

recentlyproducedspectacularresults,suchasbeating

worldchampionsinboardgames
[14,15],reaching
human-levelperformanceinsophisticatedcomputergames
]16,17]
,andcontrollingroboticlocomotion
[18,19].Applyingmodel-freeRLtoquantumcontrolimplies
directinteractionofthelearningagentwiththecontrolled

quantumsystem,whichpresentsanumberofunique

challengesnottypicallyencounteredinclassicalenviron-

ments.Quantumsystemshavelargecontinuousstate

spacesthatareonlypartiallyobservabletotheagent

throughmeasurements.Forexample,apurequbitstate
*vladimir.sivak@yale.edu
ƒmichel.devoret@yale.edu
PublishedbytheAmericanPhysicalSocietyunderthetermsof
theCreativeCommonsAttribution4.0International
license.
Furtherdistributionofthisworkmustmaintainattributionto
theauthor(s)andthepublishedarticle
™stitle,journalcitation,
andDOI.
PHYSICALREVIEWX
12,011059(2022)
2160-3308
=22=12(1)=011059(23)011059-1PublishedbytheAmericanPhysicalSociety


===== PAGE 2 =====
canbedescribedasapointonaBlochsphere,buta
projectivemeasurementofaqubitobservableyieldsa
randombinaryoutcome.Qubitsareoftenusedasancillary
systemstocontrolharmonicoscillators,inwhichcasethe

underlyingstatespaceisformallyinfinitedimensional.
Learningquantumcontrolofsuchsystemsisakinto
learningtodriveacarwithasinglesensorthatprovides
binary-valuedfeedback.Thefollowingquestionarises:Can
classicalmodel-freeRLagentsefficientlyhandlesuch
ﬁquantum-observable
ﬂenvironments?
ThepreviousapplicationsofRLtoquantumcontrol
[20Œ39],whichwesurveyinSec.
II,reliedonanumberof
simplifyingassumptionsrenderingthequantumcontrol
problemmoretractablefortheagentbutseverelylimiting
theirexperimentalfeasibility.Theseapproachesprovidethe
agentwiththeknowledgeofaquantumstateorrelyon

fidelityasameasureofoptimizationprogress.Such
requirementsareatoddswiththefundamentalproperties
ofquantumenvironments,stochasticityandminimalistic
observability.Tryingtomeettheserequirementsinrealistic
experimentsleadstoalargesamplesize,e.g.,
107mea-surementstolearnasingle-qubitgatewithonly16
parameters,asrecentlydemonstratedinRef.
[40]usingaquantum-state-awareagentthatreliedontomographyto
obtainthequantumstate.Othermodel-freeapproachesthat
viewquantumcontrolasastandardcostfunctionoptimi-
zationproblem
[6Œ10]aresubjecttosimilarlimitations.
Scalingsuchmethodsbeyondone-ortwo-qubitapplica-

tionsisprohibitivelyexpensivefromapracticalpoint
ofview.
Inthispaper,wedevelopaframeworkformodel-free
learningofquantumcontrolpolicies,whichisexplicitly
tailoredtothestochasticityandminimalisticquantum

observability.Itdoesnotrelyonrestrictiveassumptions,
suchasamodelofthesystem
™sdynamics,knowledgeofa
quantumstate,oraccesstofidelity.Byframingquantum
controlasaquantum-observableMarkovdecisionprocess
(QOMDP)[41],weconsidereachstochasticexperimental
realizationasanepisodeofinteractionofthelearningagent
withacontrolledquantumsystem,afterwhichtheagent

receivesabinary-valuedrewardthroughaprojective
measurement.Insteadofutilizingaveraging,everysuch
episodeisperformedwithadifferentcontrolpolicy,which
isbeingcontinuallyupdatedbyasmallamountwithina
trustregionwiththehelpoftherewardsignal.Thisnovel

strategyofexplorationofthepolicyspaceleadstoexcellent
sampleefficiencyonchallenginghigh-dimensionaltasks,
significantlyoutperformingwidelyusedmodel-free
methods.Toillustrateourapproachwithspecificexamples,we
focusonthequantumcontrolofaharmonicoscillator.
Harmonicoscillatorsareubiquitousphysicalsystems,
realized,forinstance,asthemotionaldegreesoffreedom
oftrappedions
[42,43]orelectromagneticmodesin
superconductingcircuits
[44,45].Theyareprimitivesfor
bosonicquantumerrorcorrection
[46Œ48]andquantum
sensing[49].Universalquantumcontrolofanoscillatoris
typicallyrealizedbycouplingittoanancillarynonlinear

system,suchasaqubit,withstate-of-the-artfidelitiesinthe

0.9Œ0.99rangeincircuitquantumelectrodynamics(QED)
[50Œ52]andtrappedions
[53].Insuchaquantumenvi-
ronment,ancillameasurementswithbinaryoutcomesare

theagent
™sonlysourceofinformationaboutthequantum
stateinthevastunobservableHilbertspaceandtheonly

sourceofrewardsguidingthelearningalgorithm.
Foranoscillator-qubitsystem,wedemonstratelearning
ofbothunitarycontrolandcontrolwithadaptivemeas-

urement-basedquantumfeedback.Thesetypesofcontrol

arespecialinstancesofamodularcircuit-basedframework,

inwhichthequantumoperationexecutedonasystemis

representedasasequenceofcontinuouslyparametrized

controlcircuits,whoseparametersarelearned
insitu
withthehelpofarewardcircuit.Weshowhowtoconstructtask-

specificrewardcircuitsthatimplementanexperimentally
feasibledichotomicpositiveoperator-valuedmeasure
(POVM)ontheoscillatorandhowtouseitsoutcomes

asrewardbitsintheclassicaltrainingloop.Wetrainthe

agenttopreparevariousnonclassicaloscillatorstates,such

asFockstates,Gottesman-Kitaev-Preskill(GKP)states

[54],Schrödingercatstates,andbinomialcodestates
[55],andtoexecutegatesonlogicalqubitsencodedin
anoscillator.
Althoughourdemonstrationisbasedonasimulated
environmentproducingmockmeasurementoutcomes,the

RLagentthatwedeveloped(codeavailableatRef.
[56])is
compatiblewithreal-worldexperiments.
II.RELATEDWORK
Inrecentyears,multipletheoreticalproposalshave
emergedaroundapplyingreinforcementlearningtoquan-

tumcontrolproblemssuchasquantumstatepreparation

[20Œ23,23Œ28]andfeedbackstabilization
[29,30],the
constructionofquantumgates
[31Œ33],designofquantum
errorcorrectionprotocols
[34Œ37],andcontrol-enhanced
quantumsensing
[38,39].Theseproposalsformulatethe
controlprobleminawaythatavoidsdirectlyfacing

quantumobservabilityandmakesitmoretractablefor

theRLagent.Insimulatedenvironments,thisispossible,

forexample,byprovidingtheagentwithfullknowledgeof

thesystem
™squantumstate,whichsuppliesenoughinfor-
mationfordecisionmaking
[20,23Œ25,27,29,34,38,39].Moreover,inthesimulation,thedistancetothetargetstate

oroperationisknownateverystepofthequantum

trajectory,anditcanbeusedtoconstructasteadyreward

signaltoguidethelearningalgorithm
[23Œ25,38],thereby
alleviatingthewell-knowndelayedrewardassignment

problem[12,13].TakingRLastepclosertowardsquantum
observability,someworksonlyprovidetheagentwith

accesstofidelitiesandexpectationvaluesofphysical
V.V.SIVAK
etal.
PHYS.REV.X
12,011059(2022)
011059-2

===== PAGE 3 =====
observablesindifferentpartsofthetrainingpipeline
[21,26,28,57,58],whichwouldstillrequireaprohibitive
amountofaveraginginanexperiment,aproblemexacer-

batedbytheiterativenatureofthetrainingprocess.Under

thesevarioussimplifications,therearepositiveindications

[23,31]thatRLisabletomatchtheperformanceof
traditionalgradient-basedmethods,albeitinsituations

wheretheagentorthelearningalgorithmhasaccessto

expensiveorunrealisticresources.Therefore,suchRL

proposalsarenotcompatiblewithefficienttrainingin
experiment,whichisrequiredinordertoeliminatemodel
biasfromquantumcontrol.Toaddressthischallenge,itis

necessarytodevelopagentsthatlearndirectlyfrom

stochasticmeasurementoutcomesorfromlow-sample

estimatorsofphysicalobservables.Initialstepstowards

thisgoalwerestudiedinRefs.
[22,30,59].III.REINFORCEMENTLEARNING
APPROACHTOQUANTUMCONTROL
A.Markovdecisionprocess
Webeginbyintroducingseveralconceptsfromthefield
ofartificialintelligence(AI).Anintelligentagentisany

devicethatcanbeviewedasperceivingitsenvironment

throughsensorsandactinguponthatenvironmentwith

actuators[60].InRL
[12,13],asubfieldofAI,the
interactionoftheagentwithitsenvironmentisusually
describedwithapowerfulframeworkofMarkovdecision

process(MDP).
IntheMDPframework,theagent-environmentinter-
actionproceedsinepisodesconsistingofasequenceof

discretetimesteps.Ateverytimestep
t,theagentreceives
anobservation
otOcontainingsomeinformationabout
thecurrentenvironmentstate
stSandactsonthe
environmentwithanaction
atA.Thisactioninduces
atransitionoftheenvironmenttoanewstate
stþ1accordingtoaMarkovtransitionfunction
Tðstþ1jst;atÞ.Theagent
selectsactionsaccordingtoapolicy
ðatjhtÞ,which,in
general,candependonthehistory
ht¼o0tofallpast
observationsmadeinthecurrentepisode.Inthepartially
observableenvironment,observationsareissuedaccording

toanobservationfunction
OðotjstÞandcarryonlylimited
informationaboutthestate.Inthespecialcaseofafully

observableenvironment,theobservation
ot¼stisa
sufficientstatisticofthepast,whichallowsustorestrict

thepolicytoamappingfromstatestoactions
ðatjstÞ.Environmentscanbefurthercategorizedasdiscreteor

continuousaccordingtothestructureofthestatespace
S,andasdeterministicorstochasticaccordingtothestructure

ofthetransitionfunction
T.Likewise,policiescanbe
categorizedasdiscreteorcontinuous,accordingtothe
structureoftheactionspace
A,andasdeterministicor
stochastic.Theagentisguidedthroughthelearningprocessbya
rewardsignal
rtR.Therewardisissuedtotheagent
aftereachaction,butitcannotbeusedbytheagentto
decideonthenextaction.Instead,itisusedbythelearning

algorithmtoimprovethepolicy.Therewardsignalis

designedbyahumansupervisoraccordingtothefinalgoal,
anditmustindicatehowgoodthenewenvironmentstateis
aftertheappliedaction.Importantly,itispossibletospecify

therewardsignalforachievingafinalgoalwithout
knowingwhattheoptimalactionsare,whichisamajor
differencebetweenreinforcementlearningandmore

widelyappreciatedsupervisedlearning.Thegoalofthe

learningalgorithmistofindapolicy
thatmaximizes
theagent
™sutilityfunction
J,whichinRListakentobethe
expectation
J¼E½Roftherewardaccumulatedduring
theepisode,alsoknownasareturn
R¼Ptrt.Evenfromthisbriefdescription,itisclearthatlearning
environmentsvaryvastlyincomplexityfrom
ﬁsimpleﬂdiscrete,fullyobservable,deterministicenvironments,such
asaRubik
™scube,to
ﬁdifficult
ﬂcontinuous,partially
observable,stochasticenvironments,suchasthoseof
self-drivingcars.Wheredoesquantumcontrollandon
thisspectrum?
B.Quantumcontrolasquantum-observable
Markovdecisionprocess
Toexplainhowquantumcontrolcanbeviewedasa
sequentialdecisionproblem,forconcretenesswespecialize
thediscussiontoatypicalcircuitQED
[45]experimental
setup,depictedinFig.
1,althoughourframeworkis
independentofthephysicalplatform.Theagentisa
programimplementedinaclassicalcomputercontrolling
thequantumsystem.Thequantumenvironmentofthe

agentconsistsofaquantumharmonicoscillator,realizedas

anelectromagneticmodeofthesuperconductingresonator,
andanancillaqubit,realizedasthetwolowestenergy
levelsofatransmon
[61].Notethedifferenceintheuseof
theterm
ﬁenvironment,
ﬂwhichinquantumphysicsrefersto
adissipativebathcoupledtoaquantumsystem,whilein
ourRLcontext,itreferstothequantumsystemitself,which
istheenvironmentoftheagent.
Itisconvenienttoabstractawaytheexactdetailsofthe
controlhardwareandadoptthecircuitmodelofquantum
control.Accordingtosuchanoperationaldefinition,the

agentinteractswiththeenvironmentbyexecutingapara-

metrizedcontrolcircuitindiscretesteps,asillustratedin
Fig.1.Oneachstep
t,theagentreceivesanobservation
otandproducestheactionvector
atofparametersofthecontrol
circuittoapplyinthenexttimestep.Theagent-environment

interactionproceedsfor
Tsteps,comprisinganepisode.
Comparedtothetypicalclassical,partiallyobservable
MDPs(POMDPs),therearetwosignificantcomplications

inthequantumcase:(i)Thequantumenvironmentis

minimallyobservabletotheagentthroughprojective
ancillameasurements;i.e.,theobservations
otcarryno
morethan1bitofinformation,and(ii)theobservation

causesarandomdiscontinuousjumpintheunderlying
MODEL-FREEQUANTUMCONTROLWITHREINFORCEMENT
–PHYS.REV.X
12,011059(2022)
011059-3

===== PAGE 4 =====
environmentstate.While,inprinciple,classicalPOMDPs
couldhavesuchproperties,theyarisemorenaturallyinthe

quantumcase.Historically,RLwassometimesbenchmarked
instochasticbutalwaysrichlyobservableenvironments,and
itisthereforeanopenquestionwhetherexistingRL

algorithmsarewellsuitedforquantumenvironmentswith

properties(i)and(ii).Thereisalsoafundamentalquestionof
whetherclassicalagentscanefficiently,inthealgorithmic
complexitysense,learncompressedrepresentationsofthe

latentquantumstatesproducingtheobservationsandifsuch
representationsarenecessaryforlearningquantumcontrol
policies.Recognizingsomeofthesedifficulties,Ref.
[41]
introducedthequantum-observableMarkovdecisionproc-
ess(QOMDP),atermwewilladopttodescribeourquantum
controlframework.
WeusetheMonteCarlowave-functionmethod
[62]tosimulatethequantumenvironmentoftheagent.Forthe
environmentconsistingofanoscillatorcoupledtoan
ancillaqubitandisolatedfromthedissipativebath,the
mostgeneralQOMDPhasthefollowingspecifications:
(1)StatespaceisthejointHilbertspaceoftheoscillator-
qubitsystem,whichinoursimulationcorrespondsto
S¼fj
siC2CN;hsjsi¼1g,with
N¼100beingtheoscillatorHilbertspacetruncationinthe
photonnumberbasis.
(2)Observationspace
O¼f−1;þ1gisasetofpossible
measurementoutcomesofthequbit
zoperator.Ifthe
controlcircuitcontainsaqubitmeasurement,the

observationfunctionisgivenbytheBornruleof

probabilities.Ifthecontrolcircuitdoesnotcontaina
measurement,theobservationisaconstant,whichwe
taketobe
ot¼þ1.Werefertotheformeras
measurement-basedfeedbackcontrolandthelatter
asunitarycontrol.
Inotherapproaches
[20,23
Œ25,27,29,34,38,39]
,an
observationisaquantumstateitself
ot¼jsti,whichis
notnaturallycompatiblewithreal-worldexperiments.
Itcouldbeobtainedthroughquantum-statetomogra-

phy
[40]
,butthiswouldresultinexponentialscalingof
thetrainingsamplecomplexitywithsystemsize.
(3)Actionspace
A¼RjAjisthespaceofparameters
aofthecontrolcircuit.Itgeneratestheset
fK½aofcontinuouslyparametrizedKrausmaps.Ifthecon-
trolcircuitcontainsaqubitmeasurement,theneach

mapK½aconsistsoftwoKrausoperators
K½asatisfyingthecompletenessrelation
Kƒþ½aKþ½aKƒ−½aK−½aIandcorrespondingtoobservations
1.Ifthecontrolcircuitdoesnotcontainameas-
urement,thenthemapconsistsofasingleunitary
operatorK0½a.(4)Statetransitionshappendeterministicallyaccording
tojstþ1i¼K0½atstiifthecontrolcircuitdoesnot
containameasurementandotherwisestochastically
accordingto
jstþ1i¼K½atsti=
pp,withproba-
bilitiesp¼hstjKƒ½atK½atsti.Inthispaper,wedonotconsiderthecouplingofa
quantumsystemtoadissipativebath,butitcanbe
incorporatedintotheQOMDPbyexpandingtheKraus
mapstoincludeuncontrolledquantumjumpsofthestate

jstiinducedbythebath.Thiswouldleadtomore
complicateddynamics,butsincethequantumstateand
itstransitionsarehiddenfromtheagent,nothingwould
changeintheRLframework.
Inthetraditionalsimulation-basedapproachtoquantum
control,themodelfor
K½aisspecified,forexample,
throughthesystem
™sHamiltonianandSchrödingerequa-
tion,allowingforgradient-basedoptimizationofthecost
function[1Œ5].Incontrast,inourapproach,theKrausmap
K½aisnotmodeled.Instead,theexperimentalapparatus
implements
K½aexactly.Inthiscase,theoptimization
proceedsatahigherlevelbytrial-and-errorlearningofthe

patternsintheaction-rewardrelationship.Thisensuresthat
thelearnedcontrolsequenceisfreeofmodelbias.
ClassicalQuantumFIG.1.Pipelineofclassicalreinforcementlearningappliedtoa
quantum-observableenvironment.Theagent(yellowbox),

whosepolicyisrepresentedwithaneuralnetwork,isaprogram

implementedinaclassicalcomputercontrollingthequantum

system.Thequantumenvironmentoftheagentconsistsofa

harmonicoscillatoranditsancillaqubit,implementedwith

superconductingcircuitsandcryogenicallycooledinthedilution

refrigerator.Thegoaloftheagentistopreparethetargetstate

jtargetioftheoscillatorafter
Ttimesteps,startingfrominitial
statej0i.Importantly,theagentdoesnothaveaccesstothe
quantumstateoftheenvironment;itcanonlyobservethe

environmentthroughintermediateprojectivemeasurementsof

theancillaqubityieldingbinaryoutcomes
ot.Theagentcontrols
theenvironmentbyproducing,ateachtimestep,theactionvector

atofparametersofthecontrolcircuit(pinkbox).Thereward
RfortheRLtrainingisobtainedbyexecutingtherewardcircuit

(bluebox)onthefinalstate
jsTipreparedineachepisode.This
circuitisdesignedtoprobabilisticallyanswerthefollowing

question:ﬁIsthepreparedstate
jsTiequalto
jtargetijgi?ﬂAbatchof
Bepisodesiscollectedpertrainingepochandusedinthe
classicaloptimizationlooptoupdatethepolicy.
V.V.SIVAK
etal.
PHYS.REV.X
12,011059(2022)
011059-4

===== PAGE 5 =====
Inpractice,commoncontributionstomodelbiascome
fromfrequency-andpower-dependentpulsedistortionsin

thecontrollines
[63,64],higher-ordernonlinearities,cou-
plingtospuriousmodes,etc.Simulation-basedapproaches

oftenattempttocompensateformodelbiasbyintroducing

additionaltermsinthecostfunction,suchaspenaltiesfor

pulsepowerandbandwidth,weightedwithsomewhat

arbitrarilychosencoefficients,orfindingpoliciesthatare

first-orderinsensitivetodeviationsinsystemparameters

[65].Incontrast,ourRLagentwilllearntherelevant
constraintsautomaticallysinceitoptimizesthetrue

unbiasedobjectiveincorporatedintothereward.
AsshowninFig.
1,therewardinourapproachis
producedbyfollowingthetrainingepisodewiththereward

circuit.ThiscircuitrealizesadichotomicPOVMonthe

oscillator,whosebinaryoutcomeprobabilisticallyindicates
whethertheappliedactionsequenceimplementsthe
desiredquantumoperation.Sincetheagent
™sgoalisto
maximizetheexpectation
J¼E½R,werequirethatinthe
statepreparationQOMDPs,therewardcircuitisdesigned

tosatisfythecondition
argmaxjiE½Rj
targeti;ð1Þwhereexpectationistakenwithrespecttothesampling
noiseinrewardmeasurementswhenthestate
jijgiissuppliedattheinputtotherewardcircuit.
IncircuitQED,dichotomicPOVMsarerealizedthrough
unitaryoperationontheoscillator-qubitsystemfollowed

byaprojectivequbitmeasurementinthe
zbasis.Sincethe
rewardmeasurement,ingeneral,willdisruptthequantum

state,weonlyapplytherewardcircuitattheendofthe

episodeandusethereward
rt<T¼0atallintermediate
timesteps.Hence,fromnowon,wewillomitthetime-step

indexandrefertotherewardassimply
RrT.Such
delayedrewardsareknowntobeparticularlychallenging

forRLagentsbecausetheyneedtomakemultipleaction

decisionsduringtheepisode,whiletherewardonlyinforms

aboutwhetherthecompletesequenceofactionswas

successfulbutdoesnotprovidefeedbackontheindividual

actions.Acommonchoiceofreward
Rinotherapproaches
[20,21,23,25Œ28,31Œ33,40]isthefidelityoftheexecuted
quantumoperation.Thefidelityoracle,oftenassumedtobe

freelyavailable,wouldtranslateintotime-consuming
averaginginexperimentsinvolvingquantumsystemswith
high-dimensionalHilbertspace,anditisthereforeprohibi-

tivelyexpensivefromapracticalpointofview.
Clearly,quantumcontrolisa
ﬁdifficult
ﬂdecisionprocess
accordingtoaroughcategorizationoutlinedinSec.
IIIA
.Onemaycompareittodrivingacarblindwithasingle

sensorthatprovidesbinary-valuedfeedbackinsteadofa

richvisualpictureofthesurroundings.Inthefollowing

subsection,wedescribeourapproachtosolvingQOMDPs

throughpolicygradientRL.
C.Solvingquantumcontrolthroughpolicygradient
reinforcementlearning
ThesolutiontoaPOMDPisapolicy
ðatjhtÞthatassignsaprobabilitydistributionoveractionstoeach
possiblehistory
ht¼o0tthattheagentmightsee.In
largeproblems,itisunfeasibletorepresentthepolicyasa
lookuptable,andinstead,itisconvenienttoparametrizeit
usingapowerfulfunctionapproximatorsuchasadeep
neuralnetwork
[14,16,66].Asanadditionalbenefit,this
representationallowsthelearningagenttogeneralizevia

parametersharingtohistoriesithasneverencountered
duringtraining.Werefertosuchneuralnetworkpoliciesas
,where
representsthenetworkparameters.Itis
advantageoustoadoptrecurrentnetworkarchitectures,
suchasthelongshort-termmemory(LSTM)
[67],in
problemswithvariable-lengthinputs.Inthiswork,weuse
neuralnetworkswithaLSTMlayerandseveralfully
connectedlayers.
Theoutputofthepolicynetworkisthemean
½htanddiagonalcovariance
2½htofthemultivariateGaussian
distributionfromwhichtheaction
atissampledonevery
timestep,asdepictedinFig.
1.Thestochasticityofthe
policyduringthetrainingensuresabalancebetween

explorationofnewactionsandexploitationofthecurrent
bestestimate
oftheoptimalaction.Typically,astraining
progresses,theagentlearnstoreducetheentropyofthe
stochasticpolicy,eventuallyconvergingtoanear-
deterministicpolicy.Afterthetrainingisfinished,the
deterministicpolicyisobtainedbychoosingtheoptimal
action.InapplicationtoQOMDPs,suchastochasticaction-
spaceexplorationstrategymeansthateveryexperimental
runisperformedwithadifferentpolicycandidate,whichis
evaluatedwithabinaryrewardmeasurement.Insteadof
spendingthesamplebudgetonincreasingtheevaluation
accuracyforanygivenpolicycandidatethroughaveraging,
ourstrategyistospendthisbudgetonevaluatingmore
policycandidates,albeitwiththeminimalaccuracy.Sucha
strategyisexplicitlytailoredtothestochasticityand

minimalisticobservabilityofquantumenvironments,and
isconceptuallyratherdifferentfromwidelyusedmodel-
freeoptimizationmethodsthatcruciallyrelyonaveraging
tosuppressnoiseinthecostfunction,aswefurtherdiscuss
intheAppendix
B.Policygradientreinforcementlearning
[12,13]provides
asetoftoolsforlearningthepolicyparameters
guidedby
therewardsignal.Eventhoughthebinary-valuedreward
Risanondifferentiablerandomvariablesampledfrom

episodicinteractionswiththeenvironment,itsexpectation
Jdependsonthepolicyparameters
,anditistherefore
differentiable.Thebasicworkingprincipleofthepolicy
gradientalgorithmsistoconstructanempiricalestimator
gkofthegradientofperformancemeasure
JðÞj¼kbasedonabatchof
Bepisodesofexperiencecollectedinthe
environmentfollowingthecurrentstochasticpolicy
k,MODEL-FREEQUANTUMCONTROLWITHREINFORCEMENT
–PHYS.REV.X
12,011059(2022)
011059-5

===== PAGE 6 =====
andthenperformagradientascentsteponthepolicy
parameters
kþ1¼kþgk,where
isthelearningrate.
Thisdatacollectionandthesubsequentpolicyupdate

compriseasingleepochoftraining.
VariouspolicygradientRLalgorithmsdifferintheir
constructionofthegradientestimator.Inthiswork,weuse

theproximalpolicyoptimizationalgorithm(PPO)
[68],whosebriefsummaryisincludedintheSupplemental
Material[69].PPOwasdevelopedtocuresuddenperfor-
mancecollapsesoftenobservedwhenusinghigh-dimen-
sionalneuralnetworkpolicies.Itachievesthisby
discouraginglargepolicyupdates(hence
ﬁproximalﬂ),inspiredbyideasfromtrustregionoptimization.The

stabilityofPPOisessentialinstochasticenvironments,
motivatingourchoiceofthisalgorithmforsolving
QOMDPs.Asdescribedabove,thelearningprocessisaguided
searchinthepolicyspace,wheretheguidingsignalisthe

rewardassignedtoeachattemptedactionsequence.Since

inthestatepreparationQOMDPthegoalistoapproach
arbitrarilyclosetothetargetstatethatresidesina
continuousstatespace,itistemptingtothinkthatthe

guidingsignalneedstobeofhighresolution,i.e.,assign

differentrewardstopoliciesofdifferentqualities,withthe
rewarddifferencebeingindicativeofthequalitydifference.
Thisconditioniscertainlysatisfiedbyusingfidelityasa

reward
[20,21,23,25Œ28,31Œ33,40].Incontrast,our
reward-circuit-basedapproachbreaksthisconditionbut
promiseshighexperimentalsampleefficiencybyvirtueof

nothavingtoperformexpensivefidelityestimation.

However,itisnotobviousthatstochastic
1outcomesoftherewardcircuitsaresufficienttonavigateacontinuous
policyspaceandconvergeatall,nottomentionreachinga

highfidelity.Forexample,considerthatfortwopolicies

withfidelities
F1>F2,inourapproach,itispossibleto
receivetherewards
R1¼−1<R2¼þ1becauseofthe
measurementsamplingnoise,leadingtotheincorrect

contributiontopolicygradient.Byprobabilisticallycom-

paringmultiplepolicycandidatesandperformingsmall
updateswithinthetrustregion,ourproximalpolicy
optimizationisabletosuccessfullycopewithsuchahighly

stochasticlearningproblem.
Thenextsectionisdevotedtoempiricallyprovingthat
ourapproachindeedleadstostablelearningconvergence,
i.e.,thattheagent
™sperformancegraduallyimprovestoa
desiredlevelanddoesnotcollapseorstagnate.We

demonstratethisbytrainingtheagenttosolvechallenging

statepreparationinstances.
Wealsoprovideasimpleintroductoryexampleillustrat-
ingthebasicprinciplesofourapproachinAppendix
A.IV.RESULTS
Currently,directpulseshapingwithgradientascentpulse
engineering(GRAPE)isadominantapproachtoquantum
statepreparationincircuitQED
[48Œ50].Nevertheless,a
modularapproachbasedonrepetitiveapplicationofa
parametrizedcontrolcircuithasseveraladvantages
[51,52].First,thankstoareducednumberofparameters,the
modularapproachislesslikelytooverfitandcangeneralize

betterundersmallenvironmentperturbations.Inaddition,

eachgateinthemodulecanbeindividuallytestedand

calibrated.Finally,themodularapproachisphysically

motivatedandmoreinterpretable,leadingtoabetter

understandingofthesolution.
OurRLapproachiscompatiblewithanyparametrized
controlcircuit,includingpiecewiseconstantparametriza-

tionusedindirectpulseshaping.Inthiswork,for

concreteness,wemaketheparticularchoiceofacontrol

circuitbasedontheuniversalgatesetconsistingofthe

selectivenumber-dependentarbitraryphasegateSNAP
ðÞanddisplacement
DðÞ[70]:SNAPðÞ¼Xn¼0einjnihnj;ð2ÞDðÞ¼expðaƒ−aÞ:ð3ÞInpractice,thisgatesethasbeenrealizedinthestrong
dispersivelimitofcircuitQED
[52,71].Displacements
DðÞareimplementedwithresonantdrivingoftheoscil-
lator,whiletheBerryphases
nintheSNAP
ðÞgateare
createdbydrivingthequbitresonantlywiththe
jgijnijeijnitransition.Recently,itwasdemonstratedthatSNAP
canbemadefirst-orderpathindependentwithrespectto

ancillaqubitdecay
[72,73].Furthermore,alinearscalingof
thecircuitdepth
Twiththestatesize
hnicanbeachieved
forthisapproach
[74],whilemanyinterestingexperimen-
tallyachievablestatescanbepreparedwithjust
T5.Inspiredbythisfinding,weparametrizeourunitarycontrol
circuitas
DƒðÞSNAPðÞDðÞ;seeFig.
2(a).InSecs.
IVA
ŒIVC
,ouraimistodemonstratethat
model-freeRLisfeasible;i.e.,thelearningconvergesto

high-fidelityprotocolsinarealisticnumberoftraining

episodes.Toisolatethelearningaspectoftheproblem,in

Secs.IVA
ŒIVC
,weuseperfectgateimplementations
actingontheHilbertspaceasintendedbyEqs.
(2)and(3).However,themajorpowerofthemodel-freeparadigm
istheabilitytoutilizeavailablecontrolsevenwhentheydo
notproducetheexpectedeffect,tailoringthelearned
actionstotheuniquecontrolimperfectionspresentinthe

system.WefocusonthisaspectinSec.
IVD
bytrainingthe
agentwithanimperfectlyimplementedSNAP.Moreover,

theadvantageofmodel-freeRLcomparedtoothermodel-

freeoptimizationmethodsisthatitcanefficientlysolve

problemsrequiringadaptivedecisionmaking
[14Œ19].We
leveragethisadvantageofRLinSec.
IVD
tolearnadaptive
measurement-basedquantumfeedbackstrategiescompen-
satingforimperfectSNAPimplementation.Finally,in
AppendixE,wedemonstratelearningofgatesforlogical
qubitsencodedinanoscillator.
V.V.SIVAK
etal.
PHYS.REV.X
12,011059(2022)
011059-6

===== PAGE 7 =====
A.PreparationofoscillatorFockstates
OnecentralquestioninourRLapproachishowtoassign
areward
Rtotheagentbyperformingameasurementon
thepreparedstate
jsTi.TosatisfyEq.
(1),itissufficientto
designtherewardcircuitinsuchawaythat
E½RfðFÞ,wherefisanymonotonouslyincreasingfunctionoffidelity
Ftothetargetstate.Althoughthisisnotnecessary,wefind
ittobeausefulguidingprinciple.Forexample,themost

efficientchoiceistogenerate
Rasanoutcomeofa
measurementwithPOVM
ftarget;I−targetg,where
target¼jihjtargetisthetargetprojector.ThisPOVM
maximizesthedistinguishabilityofthetargetstatefromall
otherstates
[75].Werefertosucharewardasthetarget
projectorreward.Ifthemeasurementoutcomesassociated

withthisPOVMare
1,thentherewardwillsatisfy
E½R2F−1.InthestrongdispersivelimitofcircuitQED
[76],adichotomicPOVMmeasurementrequiredforthetarget
projectorrewardcanberoutinelyrealizedforanimportant

classofnonclassicalstatesknownasFockstates
jni,which
areeigenstatesofthephotonnumberoperator.Tolearnthe
preparationofsuchstates,weusethe
ﬁFockrewardcircuit
ﬂshowninFig.
2(a).Allrewardcircuitsconsideredinthisworkcontaintwo
ancillameasurements.IftheSNAPisidealasinEq.
(2),thequbitwillremainin
jgiafterthecontrolsequence,and
theoutcomeofthefirstmeasurementwillalwaysbe
m1¼1,whichisthecaseinSecs.
IVA
ŒIVC
andin
AppendixE.However,inarealexperimentalsetup,
residualentanglementbetweenthequbitandoscillator
canremain.Therefore,ingeneral,thefirstmeasurement

servestodisentanglethem.Thesecondmeasurementwith
outcomem2isusedtoproducethereward.IntheFock
rewardcircuit,thisisdoneaccordingtotherule
R¼−m2.Thetrainingepisodesbeginwiththeoscillatorinvacuum
j0i¼j0iandtheancillaqubitinthegroundstate
jgi.EpisodesfollowthegeneraltemplateshowninFig.
1,in
whichthecontrolcircuitisappliedfor
T¼5timesteps,
followedbytheFockrewardcircuit.TheSNAPgateis
truncatedat
¼15levels,leadingtothe(
15þ2)-dimen-sionalparametrizationofthecontrolcircuitandamounting

to85realparametersforthefullcontrolsequence.Inour
approach,thechoiceofthecircuitdepth
Tandtheaction-
spacedimension
jAj¼þ2needstobemadein
advance,whichrequiressomepriorunderstandingofthe
problemcomplexity.Inthisexample,wechoose
T¼5and¼15forallFockstates
j1i;...;j10itoensureafair
comparisonoftheconvergencespeed,but,inprinciple,the
stateswithlower
ncanbepreparedwithshortersequences
[70,71].Anautomatedmethodforselectingthecircuit
depthwasproposedinRef.
[74],anditcanbeutilizedhere
tomakeaneducatedguessof
T.TheactionvectorsaresampledfromtheGaussian
distributionproducedbythedeepneuralnetworkwith
oneLSTMlayerandtwofullyconnectedlayers,represent-
ingthestochasticpolicy.Theneuralnetworkinputisonly
theﬁclockﬂobservation(one-hotencodingofthestepindex
t)sincetherearenomeasurementoutcomesintheunitary
controlcircuit.Theagentistrainedfor
4×103epochswith
batchesof
B¼103episodesperepoch.Thisamountstoa
samplesizeof
Mtot¼4×106experimentalruns.Thetotal
timebudgetofthetrainingissplitbetween(i)experience
collection,(ii)optimizationoftheneuralnetwork,and
(iii)communicationandinstrumentsreinitialization.We
estimatethatwiththehelpofactiveoscillatorreset
[77],the
010002000
30004000
0.90.990.9990Fidelity  Epoch(a)(b)Control circuitTReward circuit0.90.990.9990Fidelity  (c)FIG.2.PreparationofFockstates
j1i;–;j10i.(a)Parametrized
controlcircuit(pink)andFockrewardcircuit(blue).Thereward

circuitcontainsaselective
pulseonthequbit,conditionedon
having
nphotonsintheoscillator.(b)Evaluationofthetraining
progress.Thebackgroundtrajectoriescorrespondtosixrandom

seedsforeachstate;solidlinesshowthetrajectorywiththe

highestfinalfidelity.(c)Summaryofcomparisonofdifferent

model-freeapproachesonthetaskofFockstatepreparation.We

performextensivehyperparametertuningforallthreeap-

proaches,asdescribedinSec.
IVA
forRL,andinAppendix
BforNelder-Mead(NM)andsimulatedannealing(SA).All

approachesareconstrainedtothesametotalsamplesizeof
Mtot¼4×106.Thedisplayedfinalfidelityisthehighest
achievedamongsixtestedrandomseeds.
MODEL-FREEQUANTUMCONTROLWITHREINFORCEMENT
–PHYS.REV.X
12,011059(2022)
011059-7

===== PAGE 8 =====
experiencecollectiontimeinexperimentcanbeasshortas
10minutesintotalforsuchtraining(assuming
150sduty
cycleperepisode).Ourneuralnetworkisimplemented

withTensorFlow
[78]onaNVIDIATeslaV100graphics
processingunit(GPU).Thetotaltimespentupdatingthe
neuralnetworkparametersis10minutesintotalforsuch

training.Therealexperimentalimplementationwilllikely

belimitedbyinstrumentreinitialization
[9].Thistime
budgetputsourproposalwithinthereachofcurrent
technology.
Throughoutthispaper,weusethefidelity
Fonlyasan
evaluationmetrictobenchmarktheagent,anditisnotused

anywhereinthetrainingloop.Ifdesired,inexperiment,the
trainingepochscanbeperiodicallyinterleavedwithevalu-
ationepochstoperformfidelityestimation
[79,80]forthe
deterministicversionofthecurrentstochasticpolicy.Other
metricscanalsobeusedtomonitorthetrainingprogress
withoutinterruption,suchasthereturnandentropyofthe

stochasticpolicy.
TheagentbenchmarkingresultsforthisQOMDPare
showninFig.
2(b).Theyindicatethatourstochasticaction-
spaceexplorationstrategyisnotonlyabletoconvergebut
alsoyieldshigh-fidelitysolutionswithinarealisticnumber
ofexperimentalruns.Theagentwasabletoreach
F>0.99forallFockstatesand
F>0.999forFockstate
j1i.Suchstableconvergenceinastochasticsettingis
possiblewithproximalpolicyoptimizationbecauseafter
everyepoch,thepolicydistributiononlychangesbyasmall
amountwithinatrustregion.Thisworkingprincipleisin

starkcontrastwithpopularoptimizationalgorithmssuchas
theNMsimplexsearch
[6Œ8]orSA
[40],whereeach
updateofthesimplex(inNM)orthestate(inSA)canresult

inadrasticallydifferentpolicy.Asaresult,bothofthese
approachesperformpoorlyonhigh-dimensionalproblems
withthestochasticcostfunction,asshowninAppendix
BandsummarizedinFig.
2(c)
.Whenconstrainedtothesame
totalnumberofexperimentalruns
Mtot¼4×106asin
Fig.
2(b)
,NMisonlyabletofindsolutionswith
F>0.99forFockstates
j1iand
j2iandSAonlyforFockstate
j1i.Despiteitslowresolution,thetargetprojectorreward
representsthemostinformativePOVMfromtheperspec-

tiveofstatecertification
[75],anditresultsinefficient
learningofstatepreparationprotocols.However,formost
targetstates,itwillbeunfeasibletoexperimentallyimple-

mentsuchPOVMinatrustworthyway.Recallthatin
circuitQED,anydichotomicPOVMontheoscillatoris
implementedwithaunitaryoperationontheoscillator-

qubitsystemandasubsequentqubitmeasurementinthe
zbasis.Thetrustworthinessrequirementimpliesthatthis
unitaryoperationcanbeindependentlycalibratedtohigh

accuracybecauseerrorsinitsimplementationcanbiasthe
rewardcircuitand,asaresult,biasthelearningobjective
oftheagent.Forexample,intheFockrewardcircuitin

Fig.2(a),theunitaryisasimplephoton-number-selective
qubitflipwhosecalibrationisrelativelystraightforward.
Therefore,weconsidertheFockrewardasafeasibleand
trustworthyinstanceofthetargetprojectorreward.
Inamoregeneralcase,whenatargetprojectorrewardis
unfeasibletoimplement,considerthefollowingprobabi-
listicmeasurementstrategy.Let
fkgbeaparametrizedset
ofPOVMelementsthatcanberealizedinatrustworthy
way.Toimplementarewardmeasurement,ineachepisode,

wefirstsampletheparameter
kfromsomeprobability
distribution
PðkÞandthenimplementadichotomicPOVM
fk;I−kgwithassociatedreward
RRk.Onecan
viewsucharewardschemeasprobabilisticallytesting
differentpropertiesofthepreparedstate,insteadoftesting
directlywhetheritisequaltothetargetstate.Thescale
Rkofthebinaryrewardischosenaccordingtotheimportance
ofeachsuchproperty.Notethatinsucharewardscheme,
theexpectationinEq.
(1)istakenwithrespecttoboththe
samplingofPOVMsandthesamplingofmeasurement
outcomes.
InSecs.
IVB
andIVC
,weconsiderexamplesofsuch
probabilisticrewardmeasurementschemes,withfurther
examplesrelevantforotherphysicalsystemsincludedin
AppendixD.B.Preparationofstabilizerstates
Theclassofstabilizerstatesisofparticularinterestfor
quantumerrorcorrection
[81].Astateisastabilizerstateif
itisauniquejointeigenvalue-1eigenstateofacommutative

stabilizergroup.Todemonstratelearningstabilizerstate
preparationinanoscillator,wetraintheagenttopreparea
gridstate,alsoknownastheGottesman-Kitaev-Preskill
(GKP)state
[54].Gridstateswereoriginallyintroducedfor
encodinga2Dqubitsubspaceintoaninfinite-dimensional

Hilbertspaceofanoscillatorforbosonicquantumerror
correction,andtheyweresubsequentlyrecognizedtobe
valuableresourcesforvariousotherquantumapplications.
Inparticular,the1Dversionofthegridstate,whichwe
considerhere,canbeusedforsensingbothrealand

imaginarypartsofadisplacementsimultaneously
[82,83].Aninfinite-energy1DgridstateisaDiraccomb
jGKP
0iPtZDðt
pÞj0xi,where
j0xiisapositioneigen-
statelocatedat
x¼0.Thegeneratorsofastabilizergroupfor
suchastateare
Sx;0¼Dð
pÞand
Sp;0¼Dði
pÞ.The
finite-energyversionofthisstate
jGKP
icanbeobtained
withgenerators
Sx;¼ESx;0E−1and
Sp;¼ESp;0E−1,where
E¼exp
ð−2aƒaÞistheenvelopeoperatorand
determinesthedegreeofsqueezinginthepeaksoftheDirac
combandtheextentofthegridenvelope.
TolearnthepreparationofsuchaGKPstate,considera
probabilisticrewardmeasurementschemebasedonaset
fkg,with
k¼x,pofPOVMelements,whicharethe
projectorsontothe
þ1eigenspacesofstabilizergenerators
Sx=p;.Thedirectionofthestabilizerdisplacement(along
xorpquadrature)issampleduniformly,andthescaleof
rewardis
Rk¼1foreachdirection.Inthisscheme,thereis
V.V.SIVAK
etal.
PHYS.REV.X
12,011059(2022)
011059-8

===== PAGE 9 =====
nosimplerelationbetween
E½RandF,butthecondition
(1)issatisfied.Incontrast,foramultiqubitsystemwitha
finitestabilizergroup,itispossibletoconstructaschemein

whichtheexpectationofrewardisamonotonousfunction

offidelitybysamplinguniformlyfromthefullstabilizer

group(seeAppendix
D).Theinfinite-energystabilizers
Sx=p;0areunitaryandcanbe
measuredintheoscillator-qubitsystemwiththestandard
phaseestimationcircuit
[84]
,aswasexperimentallydem-
onstratedwithtrappedions
[85]
andsuperconductingcircuits
[86]
.Ontheotherhand,thefinite-energystabilizers
Sx=p;arenotunitarynorHermitian.Recently,anapproximate
circuitforgeneralizedmeasurementof
Sx=p;wasproposed
[87,88]
andrealizedwithtrappedions
[88]
.Ourstabilizer
rewardcircuit,showninFig.
3(a)
,isbasedonthese
proposals.Themeasurementoutcome
m2,obtainedinthis
circuit,isadministeredasareward
R¼m2.Sincethiscircuit
onlyapproximatesthedesiredPOVM,sucharewardwill

onlyapproximatelysatisfy
E½Rh
Sx;iþhSp;iÞ=2andfulfillthecondition
(1)
.Nevertheless,theagentthatstrivesto
maximizesucharewardwilllearntoprepareanapproximate
jGKP
istate.
Afterchoosingtherewardcircuit,weneedtoproperly
constrainthecontrolcircuit.Gridstateshavealargephoton

numbervariance

varðnÞphni1=ð22Þ;hence,prepa-
rationofsuchstatesrequiresalargeSNAPtruncation
.However,increasingtheaction-spacedimension
jAj¼þ2canresultinlessstableandefficientlearning.As
acompromise,wechoose
¼30andT¼9,amountingto
288realparametersforthefullcontrolsequence.
TheagentbenchmarkingresultsforthisQOMDPare
showninFig.
3(b),withtheaveragestabilizervalueasthe
evaluationmetric[measuredwiththeapproximatecircuit
fromFig.
3(a)].Foraperfectpolicy,thestabilizerswould
saturateto
þ1,butitisincreasinglydifficulttosatisfythis
requirementfortargetstateswithsmaller
becauseofa
limitedSNAPtruncationandcircuitdepth.Nevertheless,
ouragentsuccessfullycopeswiththistask.Example
Wignerfunctionsofthestatespreparedbytheagentafter
10,000epochsoftrainingareshownasinsets.
Learningstatepreparationwithaprobabilisticreward
measurementschemeisgenerallylessefficientthanwitha
targetprojectorrewardbecauseindividualrewardbitscarry
onlypartialinformationaboutthestate.However,in
principle,ifstabilizermeasurementscanberealizedina
quantumnondemolitionway,thisopensapossibilityof
acquiringthevaluesofmultiplecommutingstabilizersafter
everyepisode,therebyincreasingthesignal-to-noiseratio
(SNR)oftherewardsignal.
RewardcircuitsinSecs.
IVA
andIVB
aredesignedfor
specialclassesofstates.Next,weconsiderhowtoconstruct

arewardcircuitapplicabletoarbitrarystates.
C.Preparationofarbitrarystates
Inthegeneralcase,weaimtoconstructanunbiased
estimatoroffidelity
Fbasedonameasurementschemethat
is(i)tomographicallycomplete,(ii)feasibletoimplement
inagivenexperimentalplatform,and(iii)trustworthy.The
requirement(i),incombinationwithuniversalityofthe
controlcircuit,isnecessarytoguaranteethatarbitrarystates
can,inprinciple,bepreparedwithourapproach.However,

itisnotsufficientbyitselfandneedstobesupplemented
withrequirements(ii)and(iii)toensurepractical
feasibility.
InthestrongdispersivelimitofcircuitQED,theWigner
tomographyisacanonicalexamplesatisfyingallthree
requirementsabove
[89]
.TheWignerfunctionisdefined
ontheoscillatorphasespacewithcoordinates
C,anditis
givenastheexpectationvalueofthe
ﬁdisplacedparity
ﬂoperator
WðÞ¼ð2=Þhi,where
¼DðÞDƒðÞ,and
¼eiaƒaisthephotonnumberparity.Hence,for
theprobabilisticrewardmeasurementschemebasedonthe
Wignerfunction,weconsideracontinuouslyparametrized
setofPOVMelements
fg,where
¼ðIþÞ=2isa
projectoronto
þ1(even)eigenspaceofthedisplacedparity
operator.
Next,weneedtodeterminetheprobabilitydistribution
PðÞaccordingtowhichthePOVMsaresamplesfromthe
setfgforrewardevaluation.Tothisend,wederivethe
estimatoroffidelitybasedontheMonteCarloimportance
samplingofthephasespace:
(a)(b)FIG.3.Preparationofgridstates.(a)Stabilizerrewardcircuit
forthetargetstate
jGKPi.Thecircuitmakesuseofthecondi-
tionaldisplacementgate
CDðÞ¼Dðz=2Þ.Thecontrolcircuit
isthesameasinFig.
2(a).(b)Evaluationofthetrainingprogress.
Thebackgroundtrajectoriescorrespondtosixrandomseedsfor

eachstate;solidlinesshowthetrajectorywiththehighestfinal

stabilizervalue.Inset:exampleWignerfunctionsofthestates

preparedbytheagentafter10,000epochsoftraining.
MODEL-FREEQUANTUMCONTROLWITHREINFORCEMENT
–PHYS.REV.X
12,011059(2022)
011059-9

===== PAGE 10 =====
F¼Zd2WðÞWtargetðÞð4Þ¼2EPE1PðÞWtargetðÞ;ð5Þwherepoints
aresampledaccordingtoanarbitrary
probabilitydistribution
PðÞ,whichisnonzerowhere
WtargetðÞ0.Theestimator
(5)leadstothefollowing
scheme,dubbedthe
ﬁWignerreward
ﬂ:First,thephase-
spacepoint
isgeneratedwithrejectionsampling,as
illustratedinFig.
4(b),andthenthedisplacedparity
ismeasured,correspondingtotherewardcircuitshownin
Fig.4(a).Therewardisthenassignedaccordingtotherule
R¼Rm2,where
R¼f2c=PðÞgWtargetðÞischosento
reflecttheimportanceofasampledphase-spacepoint,and
c>0isanarbitraryscalingfactor.Sucharewardsatisfies
E½RcFaccordingtoEq.
(5)butonlyrequiresasingle
binarytomographymeasurementperpolicycandidate.
Theestimator
(5)isunbiasedforany
PðÞ,butits
variancecanbereducedbychoosing
PðÞoptimally.The
lowestvarianceisachievedwith
PðÞjWtargetðÞj,asshowninAppendix
C.Suchachoicealsohelpsto
stabilizethelearningalgorithmsinceitconveniently
leadstorewards
R¼m2sgnWtargetðÞofequalmagnitude
jRj¼1,wherewemadeaproperchoiceofthescaling
factorc.Weinvestigatetheagent
™sperformancewithWigner
rewardcircuitfor(i)preparationoftheSchrödingercat

statejtargetijiþj
−iwith¼2inT¼5steps,showninFig.
4(c),and(ii)preparationofthebinomialcode
statejtargeti
3pj3iþj9i[55]inT¼8steps,shownin
Fig.4(d).Incontrasttotargetprojectorandstabilizer
rewardsthatasymptoticallyleadtoarewardof
þ1foroptimalpolicy,theWignerrewardremainsstochasticeven
undertheoptimalpolicy.Sinceinthiscaseitisimpossible

tofindthepolicythatwouldsystematicallyproducea

rewardof
þ1,forsomestates,theagentconvergesto
policiesofintermediatefidelity(greenline).Toincreasethe
SNRoftheWignerreward,weevaluateeverypolicy
candidatewithrewardcircuitscorrespondingto1,10,

and100differentphase-spacepoints,doingasingle

measurementperpointandaveragingtheobtainedmeas-
urementoutcomestogeneratethereward
R.Theresults
showthattheincreasedrewardSNRallowsustoreach

higherfidelity,albeitattheexpenseofincreasedsample

size.Weexpectthatinthelimitofinfiniteaveraging,the

trainingwouldproceedasifthefidelity
Fwasdirectly
availabletobeusedasareward(blueline).
Weobservenotablevariationsinconvergencespeedand
saturationfidelitydependingonthechoiceofhyperpara-

meters,whichistypicalofreinforcementlearning.Alotof
progresshasbeenmadeindevelopingrobustRLalgo-
rithmsapplicabletoavarietyoftaskswithoutextensive

problem-specifichyperparametertuning
[15,16],butthis
stillremainsamajoropenprobleminthefield.Thelistof
hyperparametersusedinallourtrainingexamplescanbe

foundintheSupplementalMaterial
[69].Evenwiththe
optimalchoiceofhyperparameters,thereisnorigorous

guaranteeofconvergence
Šaproblemplaguingallheuris-
ticoptimizationmethodsinnonconvexspaces.Inthe

presentedexamples,weplotlearningtrajectoriescorre-

spondingtoseveralrandomseedstodemonstratethatthe

probabilityofgettingstuckwithasuboptimalsolution

issmall.
Thisdemonstrationshowsthatarbitrary-stateprepara-
tionis,inprinciple,possiblewithourapproach,aslongasa

tomographicallycompleterewardmeasurementschemeis
1 pt10 pts100 pts(a)(c)(b)0.90.990.9990Fidelity  102103104Epoch0.90.990.9990Fidelity  1 pt10 pts100 pts(d)FIG.4.Preparationofarbitrarystates.(a)Wignerreward
circuitbasedonthemeasurementofthephotonnumberparity.

Inthiscircuit,theconditionalparitygatecorrespondsto

jgihgjIþjeihej.(b)Wignerfunctionofthecatstate
jtargetijiþj−i,with
¼2.Scatteredstarsillustrate
phase-spacesamplingofpoints
fortheWignerreward.
(c)Evaluationofthetrainingprogressforthecatstate.The

backgroundtrajectoriescorrespondtosixrandomseedsforeach

setting;solidlinesshowthetrajectorywiththehighestfinal

fidelity.TheWignerrewardisobtainedbysampling1,10,and

100differentphase-spacepoints,doingasinglemeasurementper

point,andaveragingtheobtainedmeasurementoutcomesto

improvetheresolutionandachieveahigherconvergenceceiling.

Forthebluecurves,thefidelity
Fisusedasareward,
representingtheexpectedperformanceinthelimitofinfinite

averaging.(d)Evaluationofthetrainingprogressforthebinomial
codestate
jtarget
i
3pj3iþj9i,whoseWignerfunctionis
shownintheinset.
V.V.SIVAK
etal.
PHYS.REV.X
12,011059(2022)
011059-10


===== PAGE 11 =====
availableinagivenphysicalsystem.InAppendix
D,we
providefidelityestimatorsbasedonthecharacteristic
function,enablingtrainingforarbitrary-statepreparation

intrappedionsandmultiqubitsystems.
ExamplesconsideredinSecs.
IVA
ŒIVC
alreadydem-
onstratethemodel-freeaspectofourapproachdespitethe

perfectgateimplementationsintheunderlyingsimulation
ofthequantum-stateevolution.Inthefollowingexample,
wedemonstratethisaspectmoreexplicitlybytrainingthe

agentonasystemwithimperfectSNAP.Inaddition,the

nextexamplehighlightsthepotentialofRLformeasure-
ment-basedfeedbackcontrol.
D.Learningadaptivequantumfeedback
withimperfectcontrols
ManyquantumcontrolexperimentswithcircuitQED
systemsclaimdecoherence-limitedfidelity
[50,71].The
effectofdecoherenceonthequantumoperationcanbe
decreasedbyreducingtheexecutiontime.However,this

wouldinvolvecontrolswithawiderspectrumandlarger

amplitude,pushingthesystemtothelimitswheremodel
assumptionsarenolongervalid.Therefore,suchexperi-
mentsaredecoherencelimitedinsteadofmodel-bias

limitedonlybychoice.Recentexperimentsthatpush

quantumcontroltowardsfasterimplementation
[51,52]revealthatsignificantpartsoftheerrorbudgetcannotbe
accountedforbycommonandwell-understoodtheoretical

models,makingtheproblemofmodelbiasexplicit.Model-

freeoptimizationwillbecomeanindispensabletoolto
achievehigherexperimentalfidelitydespitetheinabilityto
capturethefullcomplexityofaquantumsystemwitha

simplemodel.
Toprovideanexampleofthiseffect,weconsideragaina
SNAP-displacementcontrolsequence.Intheoscillator-

qubitsystemwithdispersivecoupling
Hc=h¼12aƒaz,theBerryphases
ninEq.
(2)arecreatedthroughphoton-
number-selectivequbitrotations:
SNAPðÞ¼XnjnihnjR−nðÞR0ðÞ;ð6ÞwhereRðÞ¼expf−ið=2Þ½cosxþsiny.Note
thatthisoperation,ifimplementedperfectly,wouldreturn
thequbittothegroundstate,andhenceitcanbeconsidered

asanoperationontheoscillatoralone,asdefinedinEq.
(2).Suchanimplementationreliesontheabilitytoselectively
addressnumber-splitqubittransitions,whichrequires
pulsesoflongduration
1=.Inpractice,itisdesirable
tokeepthepulsesshorttoreducetheprobabilityofancilla
relaxationduringthegate.However,shorterpulsesofwider
bandwidthwoulddriveunintendedtransitions,asillus-

tratedinFig.
5(b),leadingtoimperfectimplementationof
theSNAPgate:InadditiontoaccumulatingincorrectBerry
phasesfordifferentlevels,thiswillgenerallyleavethe
qubitandoscillatorentangled.Suchimperfectionsare
notoriouslydifficulttocalibrateoutorpreciselyaccount
foratthepulseorsequenceconstructionlevel,which

presentsagoodtestbedforourmodel-freelearning

paradigm.Wedemonstratethatourapproachleadsto

high-fidelityprotocolseveninthecase
<1=farfrom
thetheoreticallyoptimalregime,wherethesequences

producedassumingidealSNAPyieldpoorfidelitybecause
ofseveremodelbias.
WebeginbyillustratinginFig.
5(a)thedegradationof
performanceofthepoliciesoptimizedforpreparationof
Fockstate
j3iusingtheunitarycontrolcircuitfromFig.
2(a)withanidealSNAP(blueline),whentestedwitha
finite-durationgateSNAP
(redandpinklines)whose
detailsareincludedintheSupplementalMaterial
[69].Achievingextremelyhighfidelity(blueline)requires

delicateadjustmentofthecontrolparameters,butthis
fine-tuningisfutilewhentheremaininginfidelityissmaller
thantheperformancegapduetomodelbias,shownwith

arrowsinFig.
2(a)andapriori
unknown.Asseenby
testingonthe
¼3.4case(redline),anyprogressthatthe
optimizermadeafter300epochswasduetooverfittingto

themodeloftheidealSNAP.Asdepictedwithaspectrum
inFig.
5(b)
,thequbitpulseofsuchdurationisstillreasonably
selective(andisclosetotheexperimentalchoice
4inRef.
[71]
),butitalreadyrequiresamuchmoresophisticated
modelingoftheSNAPimplementationinordertonotlimit
theexperimentalperformance.Inthepartiallyselectivecase

¼0.4(pinkline),theperformanceisdrasticallyworse.
Notethatsequencesoptimizedwithanyothersimulation-
basedapproachassumingidealSNAP,suchasRefs.
[70,74]
,wouldexhibitasimilardegradation.
Onewaytorecoverhigherfidelityisthroughadetailed
modelingofthecompositequbitpulseintheSNAP
[52],althoughsuchanapproachwillstillcontainresidualmodel
bias.Analternativeapproach,whichcomesattheexpense
ofreducedsuccessrate,istoperformaverificationancilla

measurementandpostselection,leadingtoacontrolcircuit,
showninFig.
5(c).Postselectingonaqubitmeasuredin
jgiinalltimesteps(history
hT¼11111)significantlyboosts
thefidelityofabiasedpolicyfrom0.9to0.97inthecase

¼3.4,butitdoesnotleadtoanyimprovementinthe
extremecase
¼0.4.Thepostselectedfidelityisstill
lowerthanwiththeidealSNAPbecausesuchascheme

onlycompensatesforqubitunder-orover-rotation,andnot
fortheincorrectBerryphases.Additionally,thetrajectories
correspondingtoothermeasurementhistorieshave

extremelypoorfidelitiesbecauseonlythehistory
hT¼11111wasobservedduringtheoptimizationwithan
idealSNAP.
However,inprinciple,ifthequbitisprojectedto
jeibythemeasurement,thedesiredstateevolutioncanstillbe
recoveredusingadaptivequantumfeedback.Experimental

Fockstatepreparationwithquantumfeedbackwasdem-

onstratedinthepioneeringworkincavityQED
[90].Inour
context,ageneralpolicyintheadaptivesettingisabinary
MODEL-FREEQUANTUMCONTROLWITHREINFORCEMENT
–PHYS.REV.X
12,011059(2022)
011059-11


===== PAGE 12 =====
decisiontree,equivalentto
2T−1distinctparametersettings
foreverypossiblemeasurementhistory.Thereexistmodel-

basedmethodsforconstructionofsuchatree
[91],butthey
arenotapplicableinthecasesdominatedby
apriori
unknowncontrolerrors.ARLagent,ontheotherhand,can

discoversuchatreeinamodel-freeway.Eventhoughour

policiesarerepresentedwithneuralnetworks,theycanbe

easilyconvertedtoadecision-treerepresentation,whichis

moreadvantageousforlow-latencyinferenceinreal-world

experimentalimplementation.
Tothisend,wetrainanewagentwithafeedback-based
controlcircuitthatdirectlyincorporatesafinite-duration

imperfectgateSNAP
,showninFig.
5(c),mimicking
traininginanexperiment.WeuseaFockrewardcircuit,

showninFig.
2(a),inwhich
m1¼1inallepisodes,despite
theimperfectSNAP,becauseofthequbitresetoperation.
Sincethecontrolcircuitcontainsameasurement,theagent

willbeabletodynamicallyadaptitsactionsduringthe

episodedependingonthereceivedoutcomes
ot.Asshown
withthegreencurvesinFig.
5(a),theagentsuccessfully
learnsadaptivestrategiesofhighfidelityeveninthe

extremecase
¼0.4.ThisindicatesthatRLisnotonly
goodforfine-tuningor
ﬁlast-mileﬂoptimization,butitis
alsoavaluabletoolforthedomainswheremodel-based

quantumcontrolisnotapplicable,e.g.,becauseofthe
absenceofreliablemodelsorprohibitivememoryrequire-
mentsforsimulationofalargeHilbertspace.
Tofurtheranalyzetheagent
™sstrategy,weselectthebest-
performingrandomseedforthecase
¼0.4after25,000
epochsoftrainingandvisualizetheresultingstateevolu-

tioninFig.
5(d).Theaveragefidelityofsuchapolicyis
F¼0.974.Therearefivehigh-probabilitybranches,allof
whichyield
F>0.9,andfurtherpostselectionofhistory
hT¼1¯1111willboostthefidelityto
F>0.999.We
observethatfidelityreducesinthebrancheswithmore

ﬁ−1ﬂmeasurementoutcomes(toptobottom)because,
beinglessprobable,suchbranchesreceivelessattention
fromtheagentduringthetraining.AsshowninFig.
5(e),toppanel,theagentchoosestofocusonlyonasmall
numberofbranches(5outof
25)andensurethattheylead
tohigh-fidelitystates.Thisisincontrasttotheprotocol
optimizedwiththeidealSNAPandtestedwithSNAP
(bottompanel),which,asaresultofmodelbias,performs

poorlyandhasrelativelyuniformprobabilityofallhistories
(ofcourse,suchprotocolwouldproduceonlyhistory
11111ifitwasappliedwithanidealSNAP).
Itisnoteworthythatinthetwomostprobablebranches
inFig.
5(e),theagentactuallyfinishespreparingthestatein
justthreestepsand,intheremainingtime,choosesto

simplyidleinsteadoffurtherentanglingthequbitwiththe
(c)(a)1-11-1-1-1-11-1-1111-1111-1=0.923=0.971p=0.010p=0.082p=0.137=0.984p=0.267=0.998p=0.477=0.9990.90.990.9990Fidelity  102103104EpochCumulative Prob., FidelityHistory0101(e)(b)(d)0.43.40.43.40.43.4Train testTrain testFIG.5.Learningadaptivemeasurement-basedquantumfeedbackforpreparationofFockstate
j3iwithimperfectcontrols.
(a)Evaluationofthetrainingprogress.Bluelines:trainingtheagentwiththeunitarycontrolcircuit,showninFig.
2(a),thatusesanideal
SNAP.Thebackgroundtrajectoriescorrespondtosixrandomseeds.Theprotocolsofthebest-performingseedarethentestedusingthe

samecontrolcircuitbutwithafinite-durationgateSNAP
substitutedinsteadofanidealSNAP.Suchatestrevealsthedegradationof
performance(redandpinklines)duetothemodelbias.(b)SpectrumofpartiallyselectivequbitpulsesusedinthegateSNAP
.The
degradationofperformanceinpanel(a)occursbecausethepulseoverlapsinthefrequencydomainwithunintendednumber-splitqubit

transitions,leavingthequbitandoscillatorentangledafterthegate.(c)Feedback-basedcontrolcircuitcontainingafinite-durationgate

SNAPandaverificationmeasurementthatproducesanobservation
otanddisentanglesthequbitandoscillator.Thequbitisalways
resetto
jgiafterthemeasurement.Thiscontrolcircuitrequireseitherpostselectionoradaptivecontrol.Theagentsuccessfullylearns
measurement-basedfeedbackcontrol(a,green)evenintheextremecase
¼0.4farfromthetheoreticallyoptimalregime
1.(d)Examplestateevolutionunderthepolicyobtainedafter25,000epochsoftraining,shownwithablackcircleinpanel(a).Theagent

choosestofocusonasmallnumberofbranchesandtoensurethattheyleadtohigh-fidelitystates.(e)Cumulativeprobabilityand

fidelityoftheobservedhistoriesquantifyingthistrend(toppanel).ThepolicytrainedwithidealSNAPandtestedwithSNAP
(bottompanel)hasrelativelyuniformprobabilityofallhistoriesandpoorfidelity.
V.V.SIVAK
etal.
PHYS.REV.X
12,011059(2022)
011059-12


===== PAGE 13 =====
oscillatorandsubjectingitselftoadditionalmeasurement
uncertainty.Intheotherbranches,thisextratimeisusedto
catchupafterpreviouslyreceivingundesiredmeasurement

outcomes.Thisindeedseemstobeanintelligentstrategy
forsuchaproblem,whichservesasapositiveindication
thatthisagentwillbeabletocopewithincoherenterrorsby

shorteningtheeffectivesequencelength.
Weemphasizethateventhoughforthisnumerical
demonstrationofmodel-freelearningwehadtobuilda

specificmodelofthefinite-durationSNAP,theagentis
completelyagnostictoitbyconstruction.Theonlyinput
thattheagentreceivesisbinarymeasurementoutcomes,

whosesourceisablackboxtotheagent.Effectively,inthis
demonstration,themodelbiascomesfromthemismatch
betweenidealandfinite-durationSNAP.Wealsotestedthe

agentagainstothertypesofmodelbias:Weadded
independentrandomstaticoffsetstotheBerryphases
andqubitrotationangles,andfoundthattheagentperforms

equallywellinthissituation.
V.DISCUSSION
AsempiricallydemonstratedinSec.
IV,ourstochastic
policyoptimizationisstableandleadstohighsample

efficiency.Startingfromarandominitialpolicy,learning
thepreparationofhigh-fidelityFockstates(withthetarget
projectorreward)andGKPstates(withthestabilizerreward)

required
106Œ107experimentalruns,andlearningwiththe
Wignerrewardrequired
107Œ108runs.Althoughseemingly
large,thissamplesizecomparesfavorablywiththenumber
ofmeasurementsrequiredtomerelytomographicallyverify
thestatesofsimilarqualityinexperiments,e.g.,
3×106forFockstates
[50]
and
2×107forGKPstates
[86]
.Exactlyquantifyingthesamplecomplexityofheuristic
learningalgorithmsremainsdifficult.However,wecan
qualitativelyestablishthegeneraltrends.Anaturalquestion
toaskiswhetherourapproachwillscalefavorablywith

increased(i)targetstatecomplexity,(ii)actionspace,and
(iii)sequencelength.
(i)Targetstatecomplexity:Sampleefficiencyoflearning
thecontrolpolicyisaffectedbymultipleinteracting

factors,butamongthemostimportantisthevariance
ofthefidelityestimatorusedfortherewardassign-
ment.VarianceoftheestimatorinEq.
(5)
with
PðÞjWtarget
ðÞjisgivenbyVar
¼4ð1þtarget
Þ2−F2,where
target
¼RjWtarget
ðÞjd−1isonemeasureof
thestatenonclassicalityknownastheWignerneg-
ativity
[92]
(seeAppendix
Cforthederivation).This
resultleadstoasimplelowerboundonthesample
complexityoflearningthestatepreparationpolicy

thatreachesthefidelity
Ftothedesiredtargetstate
M>4ð1þtarget
Þ2−F2ð1−FÞ2:ð7ÞThisexpressionboundsthenumberofmeasurements
Mrequiredforresolvingthefidelity
Fofafixed
policywithstandarderrorofthemeancomparableto
theinfidelity.ThetaskoftheRLagentismore
complicatedsinceitneedstonotonlyresolvethe

fidelityofthecurrentpolicybut,atthesametime,
learnhowtoimproveit.Therefore,thisboundisnot
tight,andthepracticaloverheaddependsonthe

choiceofcontrolparametrization,thelearningalgo-
rithm,anditshyperparameters.However,thebound
(7)clearlyindicatesthatlearningthepreparationof
largernonclassicalstatesisincreasinglydifficult,as
onewouldexpect,andthedifficultycanbequantified
accordingtotheWignernegativityofthestate.Thisis

afundamentallimitationonthelearningefficiency
withtheWignerreward,whichcanonlybeovercome
bydesigningarewardschemethattakesadvantageof

thespecialstructureofthetargetstateandavailable
trustworthystatemanipulationtools,aswedid,for
instance,forFockstatesandGKPstates.TheWigner
negativityofFockstatesgrowsas

np[92]
,where
nisthephotonnumber,whichwouldresultin
OðnÞscalingofthebound
(7)
.Incontrast,thetarget
projectorreward,ofwhichtheFockrewardisa
specialcase,hastarget-state-independentvariance
Var
¼Fð1−FÞleadingtoabound
M>F=ð1−FÞ,whichdoesnotincreasewiththephotonnumber.
Howsucharewarddesigncanbeoptimizedin
general,isamatterthatweleaveforfurtherinves-

tigation.
(ii)Actionspace:TheoverheadontopofEq.
(7)isdetermined,amongotherfactors,bythechoiceofthe
controlcircuit.InthecaseofSNAPanddisplacement,
theaction-spacedimension
jAj¼þ2hastogrow
withthetargetstatesizetoensureindividualcontrolof

thephasesofinvolvedoscillatorlevels.Thismight

beproblematicsincetheperformanceofRL(orany

otherapproach)usuallydeclinesonhigh-dimensional

tasks,asevidenced,forinstance,bystudiesofrobotic
locomotionwithdifferentnumbersofcontrollable
joints
[93,94]
.However,thesamplecomplexityisnot
asimplefunctionof
jAj,ascanbeinferredfrom
Fig.
2(b)
,whereweusethesame
jAj¼17forallFock
states.ForlowerFockstates,theagentquicklylearns

todisregardtheirrelevantactiondimensionsbecause

theircontributiontopolicygradientaveragestozero.

Incontrast,forhigherFockstates,itneedstodiscover
thepatternofrelationsbetweenallactiondimensions
acrossdifferenttimesteps,andthusthelearningis

slower.Notethatonthesameproblem,amuch

strongerdegradationisobservedwhenusingthe

Nelder-Meadapproachorsimulatedannealing[see

Fig.
2(c)
].(iii)Sequencelength:Tacklingdecision-makingprob-
lemswithlong-termdependencies(i.e.,
T1)iswhatmadeRLpopularinthefirstplace,
MODEL-FREEQUANTUMCONTROLWITHREINFORCEMENT
–PHYS.REV.X
12,011059(2022)
011059-13


===== PAGE 14 =====
asexemplifiedbyvariousgame-playingagents
[14Œ17].Inquantumcontrol,thetemporalstructure
ofthecontrolsequencescanbeexploitedbyadopting
recurrentneuralnetworkarchitectures,suchasthe
LSTMusedinourwork.Recently,machinelearning

forsequentialdatahassignificantlyadvancedwith
theinventionofthetransformermodels
[95],which
useattentionmechanismstoensurethatthegradients

donotdecaywiththesequencedepth
T.Machine-
learninginnovationssuchasthiswillundoubtedly
findapplicationsinquantumcontrol.
Ascanbeseenabove,therearesomeaspectsof
scalabilitythatarenotspecifictoquantumcontrolbut

arecommoninanycontroltask.Thegeneralityofthe
model-freereinforcementlearningframeworkmakesit
possibletotransferthesolutionstosuchchallenges,found

inotherdomains,toquantumcontrolproblems.
Letusnowreturntothediscussionofotherfactors
influencingthesampleefficiency.Aswebrieflyalludedto

previously,theoverheadontopofEq.
(7)dependsonthe
learningalgorithmanditshyperparameters.Model-freeRL
isknowntobelesssampleefficientthangradient-based

methods,typicallyrequiringmillionsoftrainingepisodes
[13].Thisisespeciallytrueforon-policyRLalgorithms,
suchasPPO,sincetheydiscardthetrainingdataaftereach

policyupdate.Incontrast,off-policymethodskeepold
experiencesinthereplaybufferandlearnfromthemeven
afterthecurrentpolicyhaslongdivergedfromtheold

policyunderwhichthedatawerecollected,typically
resultinginbettersampleefficiency.OurpickofPPO
wasmotivatedbyitssimplicityandstabilityinthe

stochasticsetting,butitisworthexploringanactively

expandingcollectionofRLalgorithms
[13]andunder-
standingwhicharemostsuitableforquantum-observable
environments.
Thesampleefficiencyofmodel-freeRLinthequantum
controlsettingcanbefurtherimprovedbyutilizingthe

strengthofconventionalsimulation-basedmethods.A
straightforwardwaytoachievethiswouldbethrough
supervisedpretrainingoftheagent
™spolicyinthesimu-
lation.Suchpretrainingwouldprovideabetterinitialpoint
fortheagentsubsequentlyretrainedinthereal-world
setting.Ourpreliminarynumericalexperimentsshowthat

thisindeedprovidessignificantspeedups.
Theproposalsdiscussedaboveresolvethebias-variance
trade-offinfavorofcompletebiaselimination,necessarily
sacrificingsampleefficiency.Inthisrespect,model-free
learningisaswingintheoppositedirectionfromthe

traditionalapproachinphysicsofconstructingsparse

physicallyinterpretablemodelswithveryfewparameters
whichcanbecalibratedinexperiment.Buildingonthe
insightsfromthemachine-learningcommunity,modelbias

can,inprinciple,bestronglyreduced(noteliminated)by
learningarichlyparametrizedmodel,eitherphysically
motivated
[96,97]orneural-networkbased
[98,99],from
directinteractionwithaquantumsystem.Thelearned
modelcanthenbeusedtooptimizethecontrolpolicywith

simulation-based(notnecessarilyRL)methods.Another
promisingalternativeistousemodel-basedreinforcement
learningtechniques
[100],wheretheagentcanplanthe
actionsbyvirtuallyinteractingwithitslearnedmodelofthe

environmentwhilerefiningboththemodelandthepolicy
usingreal-worldinteractions.Finally,inadditiontoadopt-
ingexistingRLalgorithms,aworthwhiledirectionisto

designnewalgorithmstailoredtothespecificsofquantum-
observableenvironments.
VI.CONCLUSION
Addressingtheproblemofmodelbiasasaninherent
limitationofthedominantsimulation-basedapproachto
quantumcontrol,weclaimthatend-to-endmodel-free

reinforcementlearningisnotonlyafeasiblealternative,
butitisalsoapowerfultoolthatwillextendthe
capabilitiesofquantumcontroltodomainswheresimu-

lation-basedmethodsarenotapplicable.Byfocusingon
controlofaharmonicoscillatorinthecircuitQED
architecture,weexploredvariousaspectsoflearningunder

theconditionsofquantumuncertaintyandscarceobserv-

ability.Ourpolicyexplorationstrategyisexplicitlytail-
oredtothesefeaturesofthequantumlearning
environments.Wedemonstratedstablelearningdirectly

fromstochasticbinarymeasurementoutcomes,insteadof
relyingonaveragingtoeliminatestochasticityasisdone
inothermodel-freequantumcontroloptimizationmeth-

ods.Withmultiplenumericalexperiments,weconfirmed
thatsuchastrategyleadstohighfidelityandsample
efficiencyonchallengingcontroltasksthatincludeboth

theunitarycontrolandcontrolwithadaptivemeasure-
ment-basedquantumfeedback.TheRLagentthatwe
developedcanbedirectlyappliedinreal-worldexperi-

mentswithvariousphysicalsystems.
ACKNOWLEDGMENTS
WeacknowledgeahelpfuldiscussionwithThomas
Fösel.Wethanktheanonymousreviewersfortheircom-

ments,whichencouragedustomakeseveraladditions.We
thankYaleCenterforResearchComputingforproviding
computeresources.ThisresearchissupportedbyARO

underGrantNo.W911NF-18-1-0212.Theviewsand
conclusionscontainedinthisdocumentarethoseofthe
authorsandshouldnotbeinterpretedasrepresentingthe

officialpolicies,eitherexpressedorimplied,oftheArmy
ResearchOffice(ARO),ortheU.S.Government.TheU.S.
Governmentisauthorizedtoreproduceanddistribute

reprintsforGovernmentpurposesnotwithstandingany
copyrightnotationherein.
V.V.SIVAK
etal.
PHYS.REV.X
12,011059(2022)
011059-14


===== PAGE 15 =====
APPENDIXA:EDUCATIONALEXAMPLE
InthisAppendix,weanalyzeadeliberatelysimple
problemwiththepurposeofillustratingindetailvarious
componentsandstagesofthelearningprocess.
Problemsetting.
ŠConsideraqubitstatepreparation
probleminwhichtheinitialstateis
jgiandthetargetstateis
jei.Suchstatepreparationcanbeachievedwithaunitary
rotationgateparametrizedas
UðaÞ¼expð−iaxÞ,where
theoptimalsolution
a¼0.5isknowninadvance.Welet
theagentdiscoverthissolutioninamodel-freeway,
withoutknowingwhichunitaryisactuallyapplied.The
trainingepisodesconsistofasingletimestepinwhichthe

agentproducesanaction
aR,leadingtoexecutionof
controlcircuit
UðaÞ;theagentthencollectsarewardwitha
simplerewardcircuitconsistingofa
zmeasurement,as
shownintheinsetofFig.
6(a).Theresultingmeasurement
outcomemf−1;1gisusedtoissueareward
R¼−m,whichismaximizedinthetargetstate
jei,hencesatisfy-
ingEq.
(1).Actorandcritic.
ŠIneverytrainingepisode,theaction
aissampledaccordingtotheprobabilitydistributionspeci-

fiedbythepolicy.Policy
ðaÞisparametrizedwith
learnableparameters
.Inthisproblem,itisconvenient
tochooseasimpleGaussianpolicy
ðaÞ¼1
22pexp−ða−Þ222;ðA1Þwhoselearnableparametersare
¼f;2g.Thepolicy
defineshowtheagentinteractswiththeenvironment,andit

isoftenreferredtoastheactor.Anotherimportant

componentofPPOisthevaluefunction
V0,orcritic,
whichhelpstheagentassessthevalueoftheenvironment

state(seeSupplementalMaterial
[69]).Inthisexample,the
valuefunctioncanbechosenasasimplebaseline
V0¼bwithlearnableparameters
0¼fbg.Duringthetraining
process,parameters
f;2;bgareiterativelyupdated
accordingtothePPOalgorithm.
Trainingprocess.
ŠThetrainingprocess,illustratedin
Fig.6,issplitinto50epochs.Withineachepoch
k,the
parametersofthepolicyremainfixed,andtheagent

collectsabatchof
B¼30episodesofexperience,behav-
ingstochasticallyaccordingtothecurrentpolicy
kðaÞ.Figure6(a)showsthepolicydistributionforaselectedset
ofepochsandtheactionsthattheagenttriedintheepisodes

ofthecorrespondingepoch.Theinitialpolicyiswidely

distributedtoensurethattheagentcanadequatelyexplore

theactionspace.Sinceinitiallymostoftheactionsdonot

leadtohigh-fidelitystates,theagentisverylikelytoreceive

negativerewards,asshowninFig.
6(b).Aftereveryepoch,
theparametersofthestochasticpolicy
(A1)areupdated
kkþ1inawaythatutilizestheinformationcontained
intherewardsignal.Controlledbythelearningrate,these

updatesresultingraduallyshiftingtheprobabilitydensity

ofthestochasticpolicytowardsmorepromisingactions,as

seeninFig.
6(a).Afteriteratinginthismannerforseveral
epochs,thepolicybecomeslocalizednearthecorrectvalue

oftheaction,whichleadstoasignificantlyincreased

fractionofpositiverewards.Intheinitialstage,thebest

progressisachievedbyrapidlylearningtheparameter
.However,toachievehighfidelity,itisnecessarytolocalize

morefinely,andthus,inthelaterstages,theagentshrinks
thevariance
2ofthepolicy.Eventually,therearealmost
noepisodeswithanegativereward,meaningthattheagent

hasachievedgoodperformance.
Complications.
ŠThissimpleexampleillustrateshow
learningproceedsinourapproach.Morerealistic

examplescontainedinSec.
IVfollowthesamebasic
principles.Additionalcomplicationsarisefromthefollow-

ingconsiderations.
(i)Typically,theactionspace
Aishighdimensional.In
suchacase,theGaussianpolicydistributionis
definedon
RjAjinsteadof
R.(ii)Theagentcanreceiveanontrivialobservation
o,forinstance,aqubitmeasurementoutcome,which
requiresincorporatingadaptivemeasurement-
basedfeedbackintothepolicy.Insuchacase,

thepolicydistribution
ðajoÞisconditionedonthe
observation.InthecaseofaGaussianpolicy,thisis
achievedbymakingthemeanandvariancepara-
(a)(b)1EpochFIG.6.Educationalexampleofmodel-freelearning.(a)Inset:
Thetaskistopreparequbitstate
jeistartingfromstate
jgi.Episodesconsistofasingletimestep;thecontrolcircuitcontains

arotationunitary
UðaÞ,andtherewardcircuitcontainsa
measurementof
z.Mainpanel:policydistribution(solidlines)
foraselectedsetofepochs,andactionsthattheagenttriedinthe

episodesofcorrespondingepoch(dots).(b)Rewardsreceivedby

theagentintheepisodesofeveryepoch.
MODEL-FREEQUANTUMCONTROLWITHREINFORCEMENT
–PHYS.REV.X
12,011059(2022)
011059-15


===== PAGE 16 =====
metrizedfunctionsoftheobservation
f;2g¼fðoÞ;2ðoÞg.Inourwork,thesefunctionsare
chosentobeneuralnetworks.
(iii)Theepisodestypicallyconsistofmultipletimesteps.
Insuchcases,thepolicydistribution
ðajt;htÞisconditionedonthetime-stepindex
tandonthe
historyofobservations
ht¼o0treceiveduptothe
currenttimestep.Fornotationalsimplicity,we
usuallytreatthetimedependenceasimplicitand
denotethepolicyas
ðajhtÞ.APPENDIXB:ALTERNATIVEMODEL-FREE
APPROACHES
1.Qualitativecomparisonof
action-spaceexplorationstrategies
Itisinstructivetocomparetheaction-spaceexploration
strategyofourRLagenttowidelyusedmodel-free

methods.Forthiscomparison,wefocusontheNMsimplex

searchusedinmanyquantumcontrolexperiments
[6Œ8].NMandothermodel-freemethodsthatviewquantum
controlasastandardcostfunctionoptimizationproblem

exploretheactionspacebyevaluatingthecostfunctionfor

asetofpolicycandidatesandbyusingthisevaluationto

informtheselectionofthenextcandidate.InNM,thelatter

stepisdonebychoosinganewvertexofthesimplex,as

illustratedinFig.
7(a).Theeffectivenessofsuchan
approachreliesontheabilitytoreliablyapproximatethe

costfunctionlandscapebyonlysamplingitatasmall

subsetofpoints.Ingeneral,thisisdifficulttoachievein

high-dimensionalactionspacesorwhenthecostfunctionis

stochastic.Therefore,suchanapproachrequiresspendinga

largepartofthesamplebudgetonaveraging,whichlimits

thenumberofpoliciesthatitcanexploreunderthe

constraintofafixedtotalsamplesizeof
Mtotexperimental
runs.Ontheotherhand,inourRLapproach,everyexper-
imentalrun(episode)isperformedwithaslightlydifferent

policy.Theserandompolicycandidatesareassigneda

stochasticscoreof
1,resultingfromtherewardmeas-
urementoutcome.Eventhoughthevalueofthe
ﬁcostfunctionﬂisnotknowntoanysatisfyingaccuracyforanyof
thepolicycandidates,theacquiredinformationissufficient

tostochasticallymovetheGaussiandistributionofpolicy

candidatestowardsamorepromisingregionoftheaction

space,asillustratedinFig.
7(b).IncontrasttoNMthat
cruciallyreliesonaveraging,ourRLagentspendsthe

samplebudgettoeffectivelyexploreamuchlargerpartof

theactionspace.
Toconfirmthisintuition,wequantitativelycomparethe
RLagenttowidelyusedmodel-freeapproaches,theNM

simplexsearchandSA,onthetaskofFockstateprepa-
rationwhenconstrainedtothesametotalsamplesizeof
Mtot¼4×106.Theresultsofthiscomparisonareshown
inFig.
2(c),revealingthatRLindeedsignificantly
outperformsitsmodel-freealternativesintermsofsample
efficiency,especiallywhentheeffectiveproblemdimen-

sionincreases,i.e.,forhigherphotonnumbers
n.Inthe
followingsections,wedescribethenumericalexperiments
withNMandSA,performedusingtheirSciPy1.4.1

implementation
[101].2.Nelder-Meadsimplexsearch
ToensureafaircomparisonofNMwithRL,weperform
hyperparametertuningforNManddisplaythebestofthe

sixindependentoptimizationrunsforeachproblemsetting.

GiventhesimplicityoftheNMheuristicwithitssmall
+1-1RLNM+1+1+1-1-1-1-1-1-1+1+1+1+1+1+1+1+1+1+1+1-0.732+0.840+0.377= Policy space= Gaussian distribution= Simplex= Policy candidate0.90.990.9990Fidelity  020004000Iteration02000
2000 avg. per iter.
NM with access to fidelity+11+1
+(c)(a)(d)(b)FIG.7.PreparationofFockstates
j1i;–;j10iwiththeNM
simplexsearch.(a)CartoondepictionoftheNMsimplexsearch.

Onealgorithmiterationcorrespondstoanupdateofonevertexof

asimplex.Thecolor-codedvaluesofthecostfunctionhavehigh

resolution,achievedthroughaveragingofmany
1measurement
outcomes.(b)CartoondepictionofourRLapproach.Every

trainingepochconsistsofseveralepisodesexecutedwithdiffer-

entpolicycandidatesthataresampledfromaGaussiandistri-

bution.Policycandidatesareassignedalow-resolutionrewardof

1basedonasinglemeasurementoutcomeinsteadofaveraging.
(c)NMoptimizationprogresswithinfidelityusedasacost

function.Thebackgroundtrajectoriescorrespondtosixrandom

seedsforeachstate,andsolidlinesshowthetrajectorywiththe

highestfinalfidelity.(d)NMoptimizationprogresswitha

stochasticcostfunctionobtainedbyaveraging2000outcomes

oftheFockrewardcircuitshowninFig.
2(a).V.V.SIVAK
etal.
PHYS.REV.X
12,011059(2022)
011059-16


===== PAGE 17 =====
numberofhyperparameters,webelievethattheperformed
tuningisexhaustiveandthatnofurthersignificantimprove-
mentsarepossible.
First,westudytheperformanceofNMwhenitisgiven
directaccesstofidelityonthetaskofFockstateprepa-

ration.Weinitializethecontrolcircuitswithrandom
parameterswhosemagnitudeisswepttooptimizethe
NMperformance,asitisknowntobesensitivetothe

simplexinitialization.Wefindthattheoptimalinitialization
issimilartothatinRLandcorrespondstorandominitial
circuitsthatdonotsignificantlydeviatetheoscillatorstate
fromvacuum.Withthischoice,theconvergenceofNMis
showninFig.
7(c).Itexhibitsfastdegradationwith
increasingphotonnumber
n.Next,westudytheperfor-
manceofNMinthepresenceofmeasurementsampling
noise.WeconstrainNMtothesametotalsamplesize
Mtot¼4×106asusedforRL,andweoptimallysplitthe
samplebudgetbetweenalgorithmiterationsandaverages
periterationtomaximizethefinalperformance.The
convergenceofNMwith2000averagesperiterationis

showninFig.
7(d),anditcanbedirectlycomparedtothe
RLresultsinFig.
2(b),clearlyshowingtheadvantageof
RLinthestochasticsetting.
3.Simulatedannealing
WeusesimulatedannealingwiththeCauchy-Lorentz
visitingdistributionandwithoutalocalsearchonaccepted
locations,whichisasimilarversiontotherecentexperi-
ment[40].Weperformedextensivetuningofhyperpara-
meters,includingthemagnitudeoftherandomlyinitialized
controlcircuitparameters,parametersofthevisitingdis-

tribution,aswellasinitialandfinaltemperatures.The
optimizationresultswiththebestchoiceofhyperpara-
metersareshowninFig.
8,whereforeachoptimization
trajectory,weonlydisplaythebestfidelityofevery100

consecutiveiterationstoreducetheplotclutterresulting
fromtheperiodicrestartsoftheannealing.
Withdirectaccesstofidelity,asshowninFig.
8(a),the
convergenceofSAissimilartoNM,andissignificantly
slowerthantheRLagentevenwhentheagentdoesnot

haveaccesstofidelity.Next,wereplacethefidelitywithits
estimatorbasedon1000runsoftheFockrewardcircuit.
Thisnumberofrunspercostfunctionevaluationistunedto

achievethehighestperformanceundertheconstrainedtotal
samplesizeof
Mtot¼4×106.Insuchstochasticsettings,
theperformanceofSAdropssignificantly,asshownin
Fig.8(b),andisworsethanthatofbothNMandRL.
APPENDIXC:VARIANCEOFTHEFIDELITY
ESTIMATORVarianceoftheestimator
(5)isgivenby
Var¼EPE2PðÞWtargetðÞ2−EPE2PðÞWtargetðÞ2ðC1Þ¼Z4PðÞW2targetðÞd−F2;ðC2Þwherewemadethesimplifications
2¼1andEP½–R½–PðÞd.Wenowusevariationalcalculustofindthe
PðÞthatminimizesEq.
(C2)withtheconstraint
RPðÞd¼1.The
variationalderivativeisgivenby
ðVarÞ¼Zc−4P2ðÞW2targetðÞPðÞd;ðC3ÞwherecistheLagrangemultiplierfortheconstraint.From
this,wefindthattheoptimalsamplingdistributionsatisfies
PðÞjWtargetðÞj,andtheminimalvarianceis
minfVarg¼4ZjWtargetðÞjd2−F2:ðC4ÞWeconsiderthesamplingprobleminwhich
Nm¼1paritymeasurementisdoneperphase-spacepoint,andin
suchsetting,wefindanoptimalsamplingdistribution
independentofthestatethatisbeingcharacterized
Šarather
convenientpropertyfortheonlinetrainingsincetheactual
preparedstateisnotknown(onlythetargetstateisknown).
Wecanconsideradifferentproblem,inwhichboth
WðÞandWtarget
ðÞareknown,andwherethegoalistocomputethe
fidelityintegral
(4)throughMonteCarlophase-spacesam-
pling.Thiscanberelevant,forinstance,inasimulation,asan
0.90.990.9990Fidelity  Iteration1000 avg. per iter.
SA with access to fidelity
)b()a(FIG.8.PreparationofFockstates
j1i;–;j10iwithsimulated
annealing.(a)SAoptimizationprogresswithinfidelityusedasa

costfunction.Thebackgroundtrajectoriescorrespondtosix

randomseedsforeachstate,andsolidlinesshowthetrajectory

withthehighestfinalfidelity.(b)SAoptimizationprogresswitha

stochasticcostfunctionobtainedbyaveraging1000outcomesof

theFockrewardcircuitshowninFig.
2(a).MODEL-FREEQUANTUMCONTROLWITHREINFORCEMENT
–PHYS.REV.X
12,011059(2022)
011059-17


===== PAGE 18 =====
alternativetocomputingtheintegralthroughtheRiemann
sum.Insuchsetting,theoptimalconditionforthevarianceis
modifiedto
PðÞjWðÞWtarget
ðÞj.If,inaddition,the
fidelityisknowninadvancetobecloseto1,i.e.,
WðÞWtarget
ðÞ,thentheoptimalsamplingdistribution
becomes
PðÞW2target
ðÞ.Thelatterdoesnotdependon
thestatethatisbeingcharacterized,andtherefore,itcanalso
beusedintheonlinesetting,aswasproposedin

Refs.
[79,80]
.However,suchasamplingdistributionis
optimalonlyinthelimit
Nm1.Ingeneral,considerfidelityestimationbasedon
Nphase-spacepointsand
Nmparitymeasurementsperpoint,
suchthatthetotalnumberofmeasurements
N¼NNmisfixed.Underthiscondition,theoptimalchoiceis
N¼N,Nm¼1(adoptedinthiswork),inwhichcasethedistri-
bution
PðÞjWtargetðÞjisoptimal.However,becauseof
varioushardwareconstraints(e.g.,smallmemoryofthe
FPGAcontroller),insomeexperiments,itmightbe

preferredtolimit
N¼Candcompensateforitby
accumulatingmultiplemeasurementsineachphase-space
point,i.e.,
Nm¼N=C
1.Undersuchconstraints,the
optimalsamplingcorrespondsto
PðÞW2targetðÞ.APPENDIXD:OTHERREWARD
MEASUREMENTSCHEMES
InthisAppendix,wedescribehowourapproachcanbe
adaptedtothecontrolofotherphysicalsystems,focusing

specificallyonthedesignofprobabilisticrewardmeasure-

mentschemes.
1.Statepreparationintrappedions
Universalcontrolofamotionalstateofatrappedioncan
beachievedbyutilizingtheion
™sinternalelectroniclevels
asancillaqubit
[42,43].Controlpoliciesaretypically
producedwithGRAPE,butmodularconstructionsalso
exist
[102].Regardlessofthecontrolcircuitparametriza-
tion,ourRLapproachcanbeusedformodel-freelearning

ofitsparameters.Here,weproposearewardcircuitthatcan

beusedforsuchlearningintrappedions,basedonthe
characteristicfunction.
Thesymmetriccharacteristicfunctionofacontinuous-
variablesystemisdefinedas
CðÞ¼hDðÞi[103].Itis
equaltothe2DFouriertransformoftheWignerfunction,

anditisthereforetomographicallycompleteandcanbe

usedtoconstructthefidelityestimatorsimilartoEq.
(5):F¼1Zd2CðÞCtargetðÞðD1Þ¼1EPE1PðÞDðÞCtargetðÞ;ðD2ÞwherePðÞisthephase-spacesamplingdistribution.In
trappedions,thecharacteristicfunctioncanbemeasured
withphaseestimationoftheunitarydisplacementoper-
ator[53,85].Forsimplicity,wefocusonsymmetricstateswhose
characteristicfunctionisreal(e.g.,FockstatesandGKP
states),althoughtheprocedurecanbegeneralizedtoasym-
metricstates.Inthiscase,therewardcircuitissimilartothe

Wignerreward,anditisshowninFig.
9.Theconditional
displacementgate
CDðÞ,requiredforsucharewardcircuit,
istypicallycalledthe
ﬁinternal-state-dependentforce
ﬂinthe
trappedionscommunity.Notethatitwasalsorecently

realizedincircuitQED
[51,86]
.2.Multiqubitsystems
Universalcontrolofasystemof
nqubitswithHilbert
spaceofdimension
d¼2ncanbeachievedwithvarious
choicesofcontrolcircuitsthatcanbetailoredtothespecific

physicallayoutofthedevice.Werefertotheliteratureon

variationalquantumalgorithmsformoredetails
[104].Here,wefocusinsteadontherewardmeasurement
schemes.Thereexistsalargebodyofworkonquantum

statecertificationinthemultiqubitsystems
[75].OurRL
approachgreatlybenefitsfromthisworksincestate
certificationprotocolscanbedirectlyconvertedinto

probabilisticrewardmeasurementschemesforstateprepa-

rationcontrolproblems.Moreover,somestatecertification
protocolsaredirectlylinkedtofidelityestimation,which
allowsustoconstructrewardmeasurementschemes

satisfyingthecondition
E½RfðFÞ,where
fisa
monotonouslyincreasingfunctionoffidelity.Here,we
proposeastabilizerrewardbuiltonthestabilizerstate
certificationprotocol
[75]andarewardforpreparationof
arbitraryn-qubitstatesbasedonthecharacteristicfunction.
a.Stabilizerstates
Considerastabilizergroup
S¼fI;S
1;–;Sd−1ganda
correspondingparametrizedsetofPOVMelements
fkg,whichconsistsofprojectors
k¼12ðIþSkÞontothe
þ1eigenspaceofeachstabilizer,exceptforthetrivialstabilizer
I.Wesampletheparameter
k¼1;–;d−1uniformlywith
probabilities
PðkÞ¼1=ðd−1Þandwiththeassociated
identicalrewardscale
Rk¼1.Therewardof
1isissued
basedonthestabilizermeasurementoutcome.Astraightfor-

wardcalculationshowsthat,inthiscase,theexpectationof

rewardsatisfies
E½R2nF−1Þ=ð2n−1Þ,andtherefore,
italsoautomaticallysatisfiesthecondition
(1).Notethe
differencefromtheGKPstatepreparationexample
FIG.9.Rewardcircuitforlearningpreparationofarbitrary
symmetricstatesofacontinuous-variablesystem,basedonthe

characteristicfunction.
V.V.SIVAK
etal.
PHYS.REV.X
12,011059(2022)
011059-18


===== PAGE 19 =====
consideredinSec.
IVB
,wherethestabilizergroupwas
infiniteandweconsideredsamplingofonlythegeneratorsof

thisgroup,whichdoesnotleadtoasimpleconnection

between
E½Rand
F.b.Arbitrarystates
Thestabilizerrewardisonlyapplicabletoarestricted
familyofstates.Toconstructarewardmeasurement

schemeapplicabletoarbitrarystates,weneedtochoose
atomographicallycompletesetofPOVMelements.The
simplestsuchschemeisbasedonthePauligroup,where

thefidelityestimatorcanbeconstructedbasedonthe

measurementsof
d2possiblen-foldtensorproducts
Gkofsingle-qubitPaulioperators
[79].Insteadofsampling
pointsinthecontinuousphasespace,inthiscase,we
sampleindices
kofthePaulioperatorsfromadiscrete
setfk¼1;–;d2gwithprobabilitydistribution
PðkÞ.Denotingthecharacteristicfunctionas
CðkÞ¼hGki,we
obtainanestimator
F¼1dXkCðkÞCtargetðkÞðD3Þ¼1dEkPE1PðkÞGkCtargetðkÞ:ðD4ÞGiventheestimatorabove,therewardcircuitsimply
consistsofmeasurementofthesampledPaulioperator.
APPENDIXE:LEARNINGGATESFOR
ENCODEDQUBITS
Thetoolsdemonstratedforquantumstatepreparationin
Sec.IVareapplicableforlearningmoregeneralquantum
operationsthatmapaninputsubspaceofthestatespaceto

thetargetoutputsubspace.Forexample,consideraqubit

encodedinoscillatorstates
fj
ZLig,whichserveas
logicalZeigenstates.Learningagate
Utargetonthislogical
qubitamountstofindinganoperationthatsimultaneously
implementsthestatetransfers
ZLiUtargetZLiandthatextendstologicalqubitsubspacebylinearity.
However,therewardcircuitsintroducedinSec.
IVwillresultinafinalstateequaltothetargetuptoanarbitrary

phasefactor;hence,itisinsufficienttoonlyusetheset

fj
ZLigduringthetraining.Toconstrainthephasefactor,
weextendthissettoincludeallcardinalpoints
fj
XLi;YLi;ZLigonthelogicalBlochsphere.
Thetrainingprocessforagateisastraightforward
generalizationofthetrainingforstatepreparationdepicted

inFig.
1,assummarizedbelow:
(1)Sampleinitialstate
j0ifj
XLi;YLi;ZLig.Starttheepisodebypreparingthisstate.
(2)Runtheepisodebyapplying
Tstepsofthecontrol
circuit,resultinginastate
jTi.(3)Applyarewardcircuittostate
jTi,withthetarget
stategivenby
jtargeti¼Utargetj0i.Here,wedemonstratelearningoflogicalgatesforthe
Fockencodingwith
jþZi¼j0iandj−Zi¼j1i,andfor
theGKPencodingwith
¼0.3.Inthesenumerical
experiments,wesampleanewinitialstateeveryepoch,
andweusethesamestateforallbatchmemberswithinthe
epoch(preparationoftheinitialstatescanbelearned

beforehand).WeuseanidealSNAP-displacementcontrol

circuit,asshowninFig.
2(a),andaWignerrewardcircuit,
asshowninFig.
4(a),withasinglephase-spacepoint
andasinglemeasurementperpolicycandidate.The

choiceoftraininghyperparametersissummarizedinthe
SupplementalMaterial
[69].ThetrainingresultsaredisplayedinFig.
10forthe
HadamardHandPauli
XgatesontheFockqubit,anda
non-Clifford

HpgateontheGKPqubit.Weuseaverage
gatefidelity
[105]asanevaluationmetric.Theseresults
showthatstableconvergenceisachievedinsuchQOMDP

despiteanadditionalsourceofrandomnessduetothe
samplingofinitialstates.Thetotalnumberofexperimental
realizationsusedbytheagentis
106,2×106,and
4×106forthe
H,X,and

Hpgates,respectively.
Infuturework,anerroramplificationtechniquebasedon
gaterepetitions,suchasrandomizedbenchmarking,canbe

incorporatedtoincreasetheSNRofthereward,similarlyto
howitisdoneinotherquantumcontroldemonstrations
[6,9].However,thistechniquecouldbemodified,inthe
spiritofourapproach,touseasingleexperimental
realizationofarandomizedbenchmarkingsequenceas
oneepisode,insteadofaveragingthemtosuppressthe

stochasticityofthecostfunction.
Epoch0.90.990Fidelity  FIG.10.Learninggatesforlogicalqubitsencodedinan
oscillator.TheagentistrainedtoproduceHadamard
HandPauliXgatesontheFockqubit,andanon-Clifford

Hpgateon
theGKPqubit.Theaveragegatefidelityisusedasanevaluation

metric.Thebackgroundtrajectoriescorrespondtosixrandom

seedsforeachgate,andsolidlinesshowthetrajectorywiththe

highestfinalfidelity.
MODEL-FREEQUANTUMCONTROLWITHREINFORCEMENT
–PHYS.REV.X
12,011059(2022)
011059-19


===== PAGE 20 =====
[1]N.Khaneja,T.Reiss,C.Kehlet,T.Schulte-Herbrüggen,
andS.J.Glaser,
OptimalControlofCoupledSpinDy-
namics:DesignofNMRPulseSequencesbyGradient

AscentAlgorithms
,J.Magn.Reson.
172,296(2005)
.[2]T.Caneva,T.Calarco,andS.Montangero,
Chopped
Random-BasisQuantumOptimization
,Phys.Rev.A
84,022326(2011)
.[3]P.DeFouquieres,S.G.Schirmer,S.J.Glaser,andI.
Kuprov,
SecondOrderGradientAscentPulseEngineer-
ing,J.Magn.Reson.
212,412(2011)
.[4]N.Leung,M.Abdelhafez,J.Koch,andD.Schuster,
SpeedupforQuantumOptimalControlfromAutomatic
DifferentiationBasedonGraphicsProcessingUnits
,Phys.Rev.A
95,042318(2017)
.[5]M.Abdelhafez,D.I.Schuster,andJ.Koch,
Gradient-BasedOptimalControlofOpenQuantumSystemsUsing
QuantumTrajectoriesandAutomaticDifferentiation
,Phys.Rev.A
99,052327(2019)
.[6]J.Kelly,R.Barends,B.Campbell,Y.Chen,Z.Chen,B.
Chiaro,A.Dunsworth,A.G.Fowler,I.C.Hoi,E.Jeffrey
etal.
,OptimalQuantumControlUsingRandomized
Benchmarking
,Phys.Rev.Lett.
112,240504(2014)
.[7]Z.Chen,J.Kelly,C.Quintana,R.Barends,B.Campbell,
Y.Chen,B.Chiaro,A.Dunsworth,A.G.Fowler,E.
Luceroetal.
,MeasuringandSuppressingQuantumState
LeakageinaSuperconductingQubit
,Phys.Rev.Lett.
116,020501(2016)
.[8]M.A.Rol,C.C.Bultink,T.E.O
™Brien,S.R.deJong,
L.S.Theis,X.Fu,F.Luthi,R.F.L.Vermeulen,J.C.de

Sterke,A.Bruno
etal.
,RestlessTuneupofHigh-Fidelity
QubitGates
,Phys.Rev.Applied
7,041001(R)(2017)
.[9]M.Werninghaus,D.J.Egger,F.Roy,S.Machnes,F.K.
Wilhelm,andS.Filipp,
LeakageReductioninFastSuper-
conductingQubitGatesviaOptimalControl
,npjQuantum
Inf.7,14(2021)
.[10]R.S.JudsonandH.Rabitz,
TeachingLaserstoControl
Molecules,Phys.Rev.Lett.
68,1500(1992)
.[11]A.Lumino,E.Polino,A.S.Rab,G.Milani,N.Spagnolo,
N.Wiebe,andF.Sciarrino,
ExperimentalPhaseEstima-
tionEnhancedbyMachineLearning
,Phys.Rev.Applied
10,044033(2018)
.[12]R.S.SuttonandA.G.Barto,
ReinforcementLearning:An
Introduction(ABradfordBook,2018).
[13]V.François-Lavet,P.Henderson,R.Islam,M.G.
Bellemare,andJ.Pineau,
AnIntroductiontoDeep
ReinforcementLearning
,Found.TrendsMach.Learn.
11,219(2018)
.[14]D.Silver,A.Huang,C.J.Maddison,A.Guez,L.Sifre,G.
vandenDriessche,J.Schrittwieser,I.Antonoglou,V.

Panneershelvam,M.Lanctot
etal.
,MasteringtheGameof
GowithDeepNeuralNetworksandTreeSearch
,Nature
(London)529,484(2016)
.[15]D.Silver,T.Hubert,J.Schrittwieser,I.Antonoglou,M.
Lai,A.Guez,M.Lanctot,L.Sifre,D.Kumaran,T.Graepel
etal.
,AGeneralReinforcementLearningAlgorithmthat
MastersChess,Shogi,andGothroughSelf-Play
,Science362,1140(2018)
.[16]V.Mnih,K.Kavukcuoglu,D.Silver,A.A.Rusu,J.
Veness,M.G.Bellemare,A.Graves,M.Riedmiller,
A.K.Fidjeland,G.Ostrovski
etal.
,Human-LevelControl
throughDeepReinforcementLearning
,Nature(London)
518,529(2015)
.[17]O.Vinyals,I.Babuschkin,W.M.Czarnecki,M.Mathieu,
A.Dudzik,J.Chung,D.H.Choi,R.Powell,T.Ewalds,P.

Georgiev
etal.
,GrandmasterLevelinStarCraftIIUsing
Multi-AgentReinforcementLearning
,Nature(London)
575,350(2019)
.[18]S.Levine,C.Finn,T.Darrell,andP.Abbeel,
End-to-EndTrainingofDeepVisuomotorPolicies
,J.Mach.Learn.
Research
17,1(2015),
https://www.jmlr.org/papers/
volume17/15-522/15-522.pdf
.[19]T.Haarnoja,S.Ha,A.Zhou,J.Tan,G.Tucker,andS.
Levine,
LearningtoWalkviaDeepReinforcementLearn-
ing,ProceedingsofRobotics:ScienceandSystems
(2019),10.15607/RSS.2019.XV.011.
[20]C.Chen,D.Dong,H.X.Li,J.Chu,andT.J.Tarn,
Fidelity-BasedProbabilisticQ-LearningforControlof
QuantumSystems
,IEEETrans.NeuralNetw.Learn.Syst.
25,920(2014)
.[21]M.Bukov,A.Day,D.Sels,P.Weinberg,A.Polkovnikov,
andP.Mehta,
ReinforcementLearninginDifferentPhases
ofQuantumControl
,Phys.Rev.X
8,031086(2018)
.[22]M.Bukov,
ReinforcementLearningforAutonomous
PreparationofFloquet-EngineeredStates:Invertingthe

QuantumKapitzaOscillator
,Phys.Rev.B
98,224305
(2018)
.[23]X.-M.Zhang,Z.Wei,R.Asad,X.-C.Yang,andX.Wang,
WhenDoesReinforcementLearningStandoutinQuantum

Control?AComparativeStudyonStatePreparation
,npjQuantumInf.
5,85(2019)
.[24]R.Porotti,D.Tamascelli,M.Restelli,andE.Prati,
CoherentTransportofQuantumStatesbyDeepReinforce-
mentLearning
,Commun.Phys.
2,61(2019)
.[25]Z.An,H.-J.Song,Q.-K.He,andD.L.Zhou,
Quantum
OptimalControlofMultilevelDissipativeQuantumSys-

temswithReinforcementLearning
,Phys.Rev.A
103,012404(2021)
.[26]M.AugustandJ.M.Hernández-Lobato,in
LectureNotes
inComputerScience
(2018),Vol.11203,pp.591
Œ613,10.1007/978-3-030-02465-9_43.
[27]T.Haug,W.-K.Mok,J.-B.You,W.Zhang,C.E.Png,and
L.-C.Kwek,
ClassifyingGlobalStatePreparationvia
DeepReinforcementLearning
,Mach.Learn.
2,01LT02
(2021)
.[28]E.-J.Kuo,Y.-L.L.Fang,andS.Y.-C.Chen,
Quantum
ArchitectureSearchviaDeepReinforcementLearning
,arXiv:2104.07715
.[29]Z.T.Wang,Y.Ashida,andM.Ueda,
DeepReinforcement
LearningControlofQuantumCartpoles
,Phys.Rev.Lett.
125,100401(2020)
.[30]S.Borah,B.Sarma,M.Kewming,G.J.Milburn,andJ.
Twamley,
MeasurementBasedFeedbackQuantumCon-
trolwithDeepReinforcementLearning
,Phys.Rev.Lett.
127,190403(2021)
.[31]M.Dalgaard,F.Motzoi,J.J.Sørensen,andJ.Sherson,
GlobalOptimizationofQuantumDynamicswithAlpha-
ZeroDeepExploration
,npjQuantumInf.
6,6(2020)
.[32]M.Y.Niu,S.Boixo,V.N.Smelyanskiy,andH.Neven,
UniversalQuantumControlthroughDeepReinforcement

Learning
,npjQuantumInf.
5,33(2019)
.V.V.SIVAK
etal.
PHYS.REV.X
12,011059(2022)
011059-20


===== PAGE 21 =====
[33]Z.AnandD.L.Zhou,
DeepReinforcementLearningfor
QuantumGateControl
,Europhys.Lett.
126,60002
(2019).[34]T.Fösel,P.Tighineanu,T.Weiss,andF.Marquardt,
ReinforcementLearningwithNeuralNetworksforQuan-
tumFeedback
,Phys.Rev.X
8,031084(2018)
.[35]P.Andreasson,J.Johansson,S.Liljestrand,andM.
Granath,QuantumErrorCorrectionfortheToricCode
UsingDeepReinforcementLearning
,Quantum3,183
(2019).[36]H.P.Nautrup,N.Delfosse,V.Dunjko,H.J.Briegel,and
N.Friis,
OptimizingQuantumErrorCorrection
CodeswithReinforcementLearning
,Quantum
3,215
(2019).[37]L.D.Colomer,M.Skotiniotis,andR.Muñoz-Tapia,
ReinforcementLearningforOptimalErrorCorrection
ofToricCodes
,Phys.Lett.A
384,126353(2020)
.[38]H.Xu,J.Li,L.Liu,Y.Wang,H.Yuan,andX.Wang,
GeneralizableControlforQuantumParameterEstimation

throughReinforcementLearning
,npjQuantumInf.
5,82
(2019).[39]J.Schuff,L.J.Fiderer,andD.Braun,
Improvingthe
DynamicsofQuantumSensorswithReinforcementLearn-

ing,NewJ.Phys.
22,035001(2020)
.[40]Y.Baum,M.Amico,S.Howell,M.Hush,M.Liuzzi,P.
Mundada,T.Merkh,A.R.Carvalho,andM.J.Biercuk,
ExperimentalDeepReinforcementLearningforError-
RobustGate-SetDesignonaSuperconductingQuantum
Computer,PRXQuantum
2,040324(2021)
.[41]J.Barry,D.T.Barry,andS.Aaronson,
QuantumPartially
ObservableMarkovDecisionProcesses
,Phys.Rev.A
90,032311(2014)
.[42]D.Leibfried,R.Blatt,C.Monroe,andD.Wineland,
QuantumDynamicsofSingleTrappedIons
,Rev.Mod.
Phys.75,281(2003)
.[43]C.D.Bruzewicz,J.Chiaverini,R.McConnell,andJ.M.
Sage,Trapped-IonQuantumComputing:Progressand
Challenges,Appl.Phys.Rev.
6,021314(2019)
.[44]P.Krantz,M.Kjaergaard,F.Yan,T.P.Orlando,S.
Gustavsson,andW.D.Oliver,
AQuantumEngineer
™sGuidetoSuperconductingQubits
,Appl.Phys.Rev.
6,021318(2019)
.[45]A.Blais,A.L.Grimsmo,S.M.Girvin,andA.Wallraff,
CircuitQuantumElectrodynamics
,Rev.Mod.Phys.
93,025005(2021)
.[46]N.Ofek,A.Petrenko,R.Heeres,P.Reinhold,Z.Leghtas,
B.Vlastakis,Y.Liu,L.Frunzio,S.M.Girvin,L.Jiang
etal.
,ExtendingtheLifetimeofaQuantumBitwithError
CorrectioninSuperconductingCircuits
,Nature(London)
536,441(2016)
.[47]P.Campagne-Ibarcq,A.Eickbusch,S.Touzard,E.Zalys-
Geller,N.E.Frattini,V.V.Sivak,P.Reinhold,S.Puri,S.
Shankar,R.J.Schoelkopf
etal.
,QuantumErrorCorrec-
tionofaQubitEncodedinGridStatesofanOscillator
,Nature(London)
584,368(2020)
.[48]L.Hu,Y.Ma,W.Cai,X.Mu,Y.Xu,W.Wang,Y.Wu,H.
Wang,Y.P.Song,C.-L.Zou
etal.
,QuantumError
CorrectionandUniversalGateSetOperationona
BinomialBosonicLogicalQubit
,Nat.Phys.
15,503
(2019).[49]W.Wang,Y.Wu,Y.Ma,W.Cai,L.Hu,X.Mu,Y.Xu,Z.-J.
Chen,H.Wang,Y.P.Song
etal.
,Heisenberg-Limited
Single-ModeQuantumMetrologyinaSuperconducting

Circuit
,Nat.Commun.
10,4382(2019)
.[50]R.W.Heeres,P.Reinhold,N.Ofek,L.Frunzio,L.Jiang,
M.H.Devoret,andR.J.Schoelkopf,
Implementinga
UniversalGateSetonaLogicalQubitEncodedinan
Oscillator
,Nat.Commun.
8,94(2017)
.[51]A.Eickbusch,V.Sivak,A.Z.Ding,S.S.Elder,S.R.Jha,
J.Venkatraman,B.Royer,S.M.Girvin,R.J.Schoelkopf,

andM.H.Devoret,
FastUniversalControlofanOscil-
latorwithWeakDispersiveCouplingtoaQubit
,arXiv:2111.06414
.[52]M.Kudra,M.Kervinen,I.Strandberg,S.Ahmed,M.
Scigliuzzo,A.Osman,D.P.Lozano,G.Ferrini,J.Bylander,
A.F.Kockum
etal.
,RobustPreparationofWigner-
NegativeStateswithOptimizedSNAP-DisplacementSe-

quences
,arXiv:2111.07965
.[53]C.FlühmannandJ.P.Home,
DirectCharacteristic-Func-
tionTomographyofQuantumStatesoftheTrapped-
IonMotionalOscillator
,Phys.Rev.Lett.
125,043602
(2020)
.[54]D.Gottesman,A.Kitaev,andJ.Preskill,
EncodingaQubit
inanOscillator
,Phys.Rev.A
64,012310(2001)
.[55]M.H.Michael,M.Silveri,R.T.Brierley,V.V.Albert,J.
Salmilehto,L.Jiang,andS.M.Girvin,
NewClassof
QuantumError-CorrectingCodesforaBosonicMode
,Phys.Rev.X
6,031006(2016)
.[56]See
https://github.com/v-sivak/quantum-control-rl
.[57]A.Garcia-SaezandJ.Riu,
QuantumObservablesfor
continuouscontroloftheQuantumApproximateOptimi-
zationAlgorithmviaReinforcementLearning
,arXiv:
1911.09682
.[58]M.M.Wauters,E.Panizon,G.B.Mbeng,andG.E.
Santoro,
Reinforcement-Learning-AssistedQuantumOpti-
mization
,Phys.Rev.Research
2,033446(2020)
.[59]M.Bilkis,M.Rosati,R.M.Yepes,andJ.Calsamiglia,
Real-TimeCalibrationofCoherent-StateReceivers:

LearningbyTrialandError
,Phys.Rev.Research
2,033295(2020)
.[60]S.RusselandP.Norvig,
ArtificialIntelligence:AModern
Approach
,4thed.(Pearson,2020),
https://www.amazon
.com/Artificial-Intelligence-A-Modern-Approach/dp/
0134610997
.[61]J.Koch,T.M.Yu,J.Gambetta,A.A.Houck,D.I.
Schuster,J.Majer,A.Blais,M.H.Devoret,S.M.Girvin,
andR.J.Schoelkopf,
Charge-InsensitiveQubitDesign
DerivedfromtheCooperPairBox
,Phys.Rev.A
76,042319(2007)
.[62]K.Mølmer,Y.Castin,andJ.Dalibard,
MonteCarloWave-
FunctionMethodinQuantumOptics
,J.Opt.Soc.Am.B
10,524(1993)
.[63]M.Jerger,A.Kulikov,Z.Vasselin,andA.Fedorov,
InSitu
CharacterizationofQubitControlLines:AQubitasa
VectorNetworkAnalyzer
,Phys.Rev.Lett.
123,150501
(2019)
.[64]M.A.Rol,L.Ciorciaro,F.K.Malinowski,B.M.
Tarasinski,R.E.Sagastizabal,C.C.Bultink,Y.Salathe,
N.Haandbaek,J.Sedivy,andL.Dicarlo,
Time-Domain
CharacterizationandCorrectionofOn-ChipDistortionof
MODEL-FREEQUANTUMCONTROLWITHREINFORCEMENT
–PHYS.REV.X
12,011059(2022)
011059-21


===== PAGE 22 =====
ControlPulsesinaQuantumProcessor
,Appl.Phys.Lett.
116,054001(2020)
.[65]T.Propson,B.E.Jackson,J.Koch,Z.Manchester,and
D.I.Schuster,
RobustQuantumOptimalControlwith
TrajectoryOptimization
,Phys.Rev.Applied
17,014036
(2022).[66]V.Mnih,K.Kavukcuoglu,D.Silver,A.Graves,I.
Antonoglou,D.Wierstra,andM.Riedmiller,
PlayingAtari
withDeepReinforcementLearning
,arXiv:1312.5602
.[67]S.HochreiterandJ.Schmidhuber,
LongShort-Term
Memory,NeuralComput.
9,1735(1997)
.[68]J.Schulman,F.Wolski,P.Dhariwal,A.Radford,andO.
Klimov,
ProximalPolicyOptimizationAlgorithms
,arXiv:
1707.06347.[69]SeeSupplementalMaterialat
http://link.aps.org/
supplemental/10.1103/PhysRevX.12.011059
forintroduc-
tiontoPPOalgorithm,implementationaldetails,andlistof
hyperparameters.
[70]S.Krastanov,V.V.Albert,C.Shen,C.-L.Zou,R.W.
Heeres,B.Vlastakis,R.J.Schoelkopf,andL.Jiang,

UniversalControlofanOscillatorwithDispersiveCou-

plingtoaQubit
,Phys.Rev.A
92,040303(R)(2015)
.[71]R.W.Heeres,B.Vlastakis,E.Holland,S.Krastanov,V.V.
Albert,L.Frunzio,L.Jiang,andR.J.Schoelkopf,
CavityStateManipulationUsingPhoton-NumberSelectivePhase
Gates,Phys.Rev.Lett.
115,137002(2015)
.[72]P.Reinhold,S.Rosenblum,W.-L.Ma,L.Frunzio,L.
Jiang,andR.J.Schoelkopf,
Error-CorrectedGatesonan
EncodedQubit
,Nat.Phys.
16,822(2020)
.[73]W.-L.Ma,M.Zhang,Y.Wong,K.Noh,S.Rosenblum,P.
Reinhold,R.J.Schoelkopf,andL.Jiang,
Path-Indepen-
dentQuantumGateswithNoisyAncilla
,Phys.Rev.Lett.
125,110503(2020)
.[74]T.Fösel,S.Krastanov,F.Marquardt,andL.Jiang,
EfficientCavityControlwithSNAPGates
,arXiv:2004.14256
.[75]M.KlieschandI.Roth,
TheoryofQuantumSystem
Certification,PRXQuantum
2,010201(2021)
.[76]D.I.Schuster,A.A.Houck,J.A.Schreier,A.Wallraff,
J.M.Gambetta,A.Blais,L.Frunzio,J.Majer,B.Johnson,

M.H.Devoret
etal.
,ResolvingPhotonNumberStatesina
SuperconductingCircuit
,Nature(London)
445,515
(2007).[77]W.Pfaff,C.J.Axline,L.D.Burkhart,U.Vool,P.Rein-
hold,L.Frunzio,L.Jiang,M.H.Devoret,andR.J.

Schoelkopf,ControlledReleaseofMultiphotonQuantum
StatesfromaMicrowaveCavityMemory
,Nat.Phys.
13,882(2017)
.[78]M.Abadi,P.Barham,J.Chen,Z.Chen,A.Davis,J.Dean,
M.Devin,S.Ghemawat,G.Irving,M.Isard
etal.
,TensorFlow:ASystemforLarge-ScaleMachineLearning
,arXiv:1605.08695
.[79]S.T.FlammiaandY.-K.Liu,
DirectFidelityEstimation
fromFewPauliMeasurements
,Phys.Rev.Lett.
106,230501(2011)
.[80]M.P.daSilva,O.Landon-Cardinal,andD.Poulin,
PracticalCharacterizationofQuantumDeviceswithout
Tomography,Phys.Rev.Lett.
107,210404(2011)
.[81]M.A.NielsenandI.L.Chuang,
QuantumComputation
andQuantumInformation
(CambridgeUniversityPress,
Cambridge,England,2010).
[82]K.Duivenvoorden,B.M.Terhal,andD.Weigand,
Single-ModeDisplacementSensor
,Phys.Rev.A
95,012305
(2017)
.[83]K.Noh,S.M.Girvin,andL.Jiang,
EncodinganOscillator
intoManyOscillators
,Phys.Rev.Lett.
125,080503
(2020)
.[84]B.M.TerhalandD.Weigand,
EncodingaQubitintoa
CavityModeinCircuitQEDUsingPhaseEstimation
,Phys.Rev.A
93,012315(2016)
.[85]C.Flühmann,T.L.Nguyen,M.Marinelli,V.Negnevitsky,
K.Mehta,andJ.P.Home,
EncodingaQubitinaTrapped-
IonMechanicalOscillator
,Nature(London)
566,513
(2019)
.[86]P.Campagne-Ibarcq,A.Eickbusch,S.Touzard,E.Zalys-
Geller,N.E.Frattini,V.V.Sivak,P.Reinhold,S.Puri,S.

Shankar,R.J.Schoelkopf
etal.
,QuantumErrorCorrec-
tionofaQubitEncodedinGridStatesofanOscillator
,Nature(London)
584,368(2020)
.[87]B.Royer,S.Singh,andS.M.Girvin,
Stabilizationof
Finite-EnergyGottesman-Kitaev-PreskillStates
,Phys.Rev.Lett.
125,260509(2020)
.[88]B.deNeeve,T.L.Nguyen,T.Behrle,andJ.Home,
ErrorCorrectionofaLogicalGridStateQubitbyDissipative
Pumping
(2022)10.1038/s41567-021-01487-7.
[89]B.Vlastakis,G.Kirchmair,Z.Leghtas,S.E.Nigg,L.
Frunzio,S.M.Girvin,M.Mirrahimi,M.H.Devoret,and

R.J.Schoelkopf,
DeterministicallyEncodingQuantum
InformationUsing100-PhotonSchrodingerCatStates
,Science
342,607(2013)
.[90]C.Sayrin,I.Dotsenko,X.Zhou,B.Peaudecerf,T.
Rybarczyk,S.Gleyzes,P.Rouchon,M.Mirrahimi,H.

Amini,M.Brune
etal.
,Real-TimeQuantumFeedback
PreparesandStabilizesPhotonNumberStates
,Nature
(London)
477,73(2011)
.[91]C.Shen,K.Noh,V.V.Albert,S.Krastanov,M.H.
Devoret,R.J.Schoelkopf,S.M.Girvin,andL.Jiang,

QuantumChannelConstructionwithCircuitQuantum

Electrodynamics
,Phys.Rev.B
95,134501(2017)
.[92]A.KenfackandK.Zyczkowski,
NegativityoftheWigner
FunctionasanIndicatorofNon-classicality
,J.Opt.B
6,396(2004)
.[93]J.Schulman,P.Moritz,S.Levine,M.Jordan,andP.
Abbeel,
High-DimensionalContinuousControlUsing
GeneralizedAdvantageEstimation
,arXiv:1506.02438
.[94]Y.Duan,X.Chen,R.Houthooft,J.Schulman,andP.
Abbeel,
BenchmarkingDeepReinforcementLearningfor
ContinuousControl
,33rdInternationalConferenceon
MachineLearning
3,2001(2016),
https://arxiv.org/abs/
1604.06778
.[95]A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,
A.N.Gomez,L.Kaiser,andI.Polosukhin,
AttentionIsAll
YouNeed
,arXiv:1706.03762
.[96]S.Krastanov,S.Zhou,S.T.Flammia,andL.Jiang,
StochasticEstimationofDynamicalVariables
,QuantumSci.Technol.
4,035003(2019)
.[97]S.Krastanov,K.Head-Marsden,S.Zhou,S.T.Flammia,
L.Jiang,andP.Narang,
UnboxingQuantumBlack
BoxModels:LearningNon-MarkovianDynamics
,arXiv:
2009.03902
.V.V.SIVAK
etal.
PHYS.REV.X
12,011059(2022)
011059-22


===== PAGE 23 =====
[98]E.Flurin,L.S.Martin,S.Hacohen-Gourgy,andI.Siddiqi,
UsingaRecurrentNeuralNetworktoReconstructQuan-
tumDynamicsofaSuperconductingQubitfromPhysical

Observations,Phys.Rev.X
10,011006(2020)
.[99]L.Banchi,E.Grant,A.Rocchetto,andS.Severini,
ModellingNon-MarkovianQuantumProcesseswithRe-
currentNeuralNetworks
,NewJ.Phys.
20,123030(2018)
.[100]A.Plaat,W.Kosters,andM.Preuss,
DeepModel-Based
ReinforcementLearningforHigh-DimensionalProblems,
ASurvey
,arXiv:2008.05598
.[101]P.Virtanen,R.Gommers,T.E.Oliphant,M.Haberland,T.
Reddy,D.Cournapeau,E.Burovski,P.Peterson,W.

Weckesser,J.Bright
etal.
,SciPy1.0:Fundamental
AlgorithmsforScientificComputinginPython
,Nat.Methods
17,261(2020)
.[102]B.KneerandC.K.Law,
PreparationofArbitraryEn-
tangledQuantumStatesofaTrappedIon
,Phys.Rev.A
57,2096(1998)
.[103]S.HarocheandJ.-M.Raimond,
ExploringtheQuantum
(OxfordUniversityPress,NewYork,2006).
[104]M.Cerezo,A.Arrasmith,R.Babbush,S.C.Benjamin,S.
Endo,K.Fujii,J.R.McClean,K.Mitarai,X.Yuan,L.

Cincio
etal.
,VariationalQuantumAlgorithms
,Nat.Rev.
Phys.
3,625(2021)
.[105]M.A.Nielsen,
ASimpleFormulafortheAverageGate
FidelityofaQuantumDynamicalOperation
,Phys.Lett.A
303,249(2002)
.MODEL-FREEQUANTUMCONTROLWITHREINFORCEMENT
–PHYS.REV.X
12,011059(2022)
011059-23
